% Divide this into 2–3 sections
% -eg “Design/Architecture” and “Implementation” details
% -... but avoid generic section titles
% Start with high-level overview of solution (top-down)
% -Give the reader the bigger picture first
%-Figure with overview of system architecture works well
%-Roadmap helps as well
%Give examples and make them consistent (eg a running example)

% todo: information if someone wants to make a resource compatible with GB: what kind of things it should expose

\chapter{Interoperability Layer for Clinical Research Systems}

In this chapter we are presenting the solution for the first objective of the project:
setting up an interoperability layer for the main open-source clinical research systems, namely tranSMART, i2b2 and their respective derivations.
The general design of the solution is first presented, then the steps that are required to implement this system are explained.

\section{Design of the Interoperability Layer}
This section describes in details the design of the solution chosen chapter~\ref{sec:sysdesign}. 
It aims to allow the cohort explorer UI Glowing Bear to support the following systems:
\begin{itemize}
    \item i2b2
    \item SHRINE 
    \item tranSMART 17.1 (REST API v2)
    \item tranSMART 16.2 (REST API v1)
\end{itemize}

Because of the fundamental differences in the APIs, it is not possible to browse at the same time. 
After being logged in, the user is able to choose from which resource do the queries.
A resource is defined as an instance of one the supported systems.

Figure~\ref{fig:sysdiagram} shows the system diagram after completion of the objective 1.
We can see that IRCT acts as a backend component for Glowing Bear, all its communications go through it using the PIC-SURE API.

\paragraph{Interoperability Layer}

\paragraph{Connectors}
what is supported for each connector
GB enable disable stuff

\paragraph{Authentication \& Authorization}
The authentication and authorization is delegated to a third-party using the OpenID Connect protocol~\cite{openidconnect}.

% note: present here solution with everything going through IRCT - if not OK in the end: change it back




Interoperability Layer for Clinical Research Systems
Common Interoperability Components
If IRCT: description of sources
Connectors
i2b2
SHRINE
(tranSMART 16.2)
(tranSMART 17.1)

exhaustive list of what

TODO: add impl step: enhance i2b2 capabilities (for counts and all), and also transmart



put the overall graph + two variants of transmart 17.1 with pic sure or not, if time allows
--> make those graphs

say that it wont query all the resources at once: some may support soem things or not 


\section{Implementation of the Interoperability Layer}
Here are described step-by-step the implementation of the solution presented in the last section. 
Each of the step first explains what is the current status of the relevant components, and then what needs to be modified or implemented.
roadmap todo
%nature of the thing-> should be test-driven


first target: most basic form, tree used to make queries


\subsection{Deploying Original Components}
First we are deploying an instance of IRCT, with an instance of i2b2 and tranSMART 16.2 which are both natively supported. 

gb
transmart 17.1

data loaded

how is it done etc.

1. irct setup + i2b2 + transmart 16.2 + shrine
deployment mainly, data? i2b2 demo data? fortransmart? 
DB setup for the systems
impl for shrine maybe? (extend the i2b2 one)

details of in appendix~\ref{sec:docker-images}


\subsection{Glowing Bear Modifications}

% GB modif overview + why no transmart rest v2
Glowing Bear is pretty heavily modified, the idea being to make it a native PIC-SURE API client by replacing completely the tranSMART client API implementation. 
Compatibility with tranSMART versions 17.1+ is later restored through the implementation of an IRCT resource interface (see~\ref{subsec:todo}).
The reasoning behind this choice is that maintaining compatibility of two different but similar APIs in Glowing Bear would be possible, but complicated, which translates into additional efforts spent on the implementation, and later on the maintenance of the code: this would be sub-optimal as these efforts are better spent elsewhere.
The potential downside of this choice is a time delay for the requests as we are introducing an additional middle-component, these will be formally measured in chapter~\ref{chap:perfeval}.

% overview of steps taken in GB workflow: steps for modifications
After reviewing what features will be removed from Glowing Bear, the organization of this section follows its typical workflow:
\begin{itemize}
    \item Client initialization: loading and login
    \item Explore the tree of concepts
    \item Step 1: Construct a query or re-use a saved query
    \item Save a constructed query
    \item Step 2: Select the data to export
    \item Step 3: Export the data
\end{itemize}

% todo: check, integrate or not
% PIC SURE vocabulary: entity / concept
% Here we will use the PIC-SURE vocabulary.
% Implementation: change the names (concept -> entity)
% Changes in Glowing Bear take root in the modification of the API calls, that are located in \verb|ResourceService|, this is thus what we take as starting points in this section. 
% STEP 1: defining the where clauses (and a few count queries)
% STEP 2: defining the SELECT clauses (what data do we want)
%Request: modify in resource service
%response: modify in models


\subsubsection{Features Removal}

% studies
The tranSMART REST API v2 has several calls that are specific to studies, while the PIC-SURE API does not have a direct equivalent to this.
However since the studies are completely embedded within the concept tree, i.e. a concept belongs to one study or it is cross-study, they can simply be abstracted into the tree of entities exposed by PIC-SURE.
It will then be the responsibility of the resource interface to deal with studies, if the resource supports it.
The following classes, functions and fields are removed:
\begin{itemize}
    \item \verb|ResourceService.getStudies()| %OK
    \item \verb|ResourceService.getCountsPerStudyAndConcept()| (replaced by getCounts())
    \item \verb|ResourceService.getCountsPerStudy()|
    \item \verb|ConstraintService.studies| %OK
    \item \verb|ConstraintService.studyConstraints| %OK
    \item \verb|TreeNodeService.isTreeNodeAStudy()| %OK
    \item \verb|GbStudyConstraintComponent| %OK
    % todo: some other things might need removal or modification
\end{itemize}

Note that the use of \verb|getCountsPerStudyAndConcept()| and \verb|getCountsPerStudy()| is replaced by the more generic \verb|ResourceService.getCounts()|.
% check the html for other things to remove

% --- text of previous version:
%The study can however be considered as a dimension of the observations, something that can be supported generically through PIC-SURE.
%Which means that this transition of studies to the more generic concept of dimension should also be applied in Glowing Bear, i.e. the behavior that is study specific should disappear.
%We rename the module \verb|gb-study-constraint| to \verb|gb-dimension-constraint| to handle constraints based on dimension, if the resource exposes such entities in its tree.
%In order to keep the name auto-completion feature, the list of possible values for the queried dimension is populated wiht the following query to PIC-SURE:
% \begin{verbatim}
% {
%   "select": [ {
%     "field": {
%       "pui": "/<resource>/<path_to_dimension_entity>",
%       "dataType": "DIMENSION"
%     },
%   "alias": "Creatinine%2C%20urine%20%28mg%20per%20dL%29"
%   } ]
% }
% \end{verbatim}


\subsubsection{Initialization}

\paragraph{Authentication \& Authorization}
The modification to Glowing Bear is the migration from the authorization protocol OAuth2~\cite{oauth2} to the authentication and authorization protocol OpenID Connect~\cite{openidconnect}.
IRCT uses OpenID Connect to authenticate and authorize the users of its client applications, while Glowing Bear originally implements the client side of the OAuth2 protocol for authentication with tranSMART. 
Since OpenID Connect is a layer on top of OAuth2, the modifications to migrate the client code from OAuth2 to OpenID Connect are not significant, and are made by making use of the \emph{oidc-client-js} library~\cite{oidc-client-js}.
The flow remains the same: Glowing Bear obtains a JSON Web Token (JWT) from the OpenID Connect provider, which will then be embedded in the header of the HTTP requests to IRCT.

\paragraph{IRCT Resources Support}

% resources discovery
After the user has successfully logged in, Glowing Bear requests the list of IRCT resources available and their properties.
Then the user chooses from the list which resource to use for the current session: it is possible to use only one resource at a time.
The selected resource and its properties are kept in an object of type \verb|IRCTResource| that stores all the information about the resource returned by PIC-SURE (see below the call to \verb|resourceService/resources|).
It also provides methods that determine if some specific features are supported, which allows some features of Glowing Bear to be enabled or disabled:
\begin{itemize}
    \item \verb|supportsCounts()|: returns \emph{true} if queries of type \verb|SELECT COUNT| are supported, which enables the live counts display in the UI
    \item \verb|supportedCountsDimensions()|: if \verb|supportsCounts()| is \emph{true}, returns the countable dimensions (most commonly \emph{Patients} and \emph{Observations}).
\end{itemize}

todo: add for the data types capabilities


% resources API call
The function \verb|ResourceService.getResources()| is added to retrieve the PIC-SURE resources. 
It does the following API call:
\begin{verbatim}
GET /rest/resourceService/resources
\end{verbatim}

Response:
\begin{verbatim}
[
  {
    "id": 1,
    "name": "resource name (e.g. i2b2-local)",
    "implementation": "resource type (which implementation is used, e.g. i2b2XML)",
    "relationships": [ supported relationships between tree nodes, e.g. CHILD ],
    "logicaloperators": [ "AND", "OR", "NOT" ],
    "predicates": [ supported predicates and their properties, e.g.: {
        "predicateName": "predicate name (e.g. CONTAINS)",
        "displayName": "displayed name (e.g. Contains)",
        "description": "description",
        "default": true if this predicate should be selected by default,
        "fields": [ {
            "name": "field name (e.g. By Encounter)",
            "path": "field code (to be used in query, e.g. ENCOUNTER)",
            "description": "By Encounter",
            "required": true,
            "dataTypes": [ data type of the value field(s) of this predicate ],
            "permittedValues": [ permitted values, if it categorical ]
          } ],
        "dataTypes": [ data types to which this predicate applies (e.g. STRING) ],
        "paths": [ paths to which this predicate applies (empty for all) ]
      } ],
    "selectOperations": [ supported operations for select (e.g. AGGREGATE) ],
    "selectFields": [ supported fields for the operations, e.g. COUNT for AGGREGATE ],
    "joins": [ ],
    "sorts": [ ],
    "processes": [ ],
    "visualization": [ ],
    "dataTypes": [ data types and their properties, e.g.: {
        "name": "name of the data type",
        "pattern": "regex to validate the value",
        "description": "description of the data type"
    } ]
  },
  ...
]
\end{verbatim}


\subsubsection{Concepts Tree}

% modification of request (api call)
The function \verb|ResourceService.getTreeNodes()| is modified according to the following:
\begin{itemize}
    \item Parameters
    \begin{itemize}
        \item \verb|depth| is removed: PIC-SURE always return a depth of 1
        \item \verb|hasCounts| is removed: PIC-SURE always return a counts field (filling it or not depend on the resource implementation); additionnally in the current implementation it is never used
        \item \verb|hasTags| is removed: PIC-SURE always return nodes metadata (content is resource dependent)
        \item \verb|relationship| is added: tree browsing is generic in PIC-SURE, relationships between nodes are defined by resource, \verb|child| and \verb|parent| at least are supported
    \end{itemize}
    \item 
\end{itemize}

The API request becomes:
\begin{verbatim}
GET /rest/resourceService/path/<resource>/<path>/?relationship=<relationship>
\end{verbatim}

Response:
\begin{verbatim}
[
  {
    "pui": "/<resource>/<path>/",
    "name": "Concept internal name",
    "displayName": "Concept name",
    "description": "Concept description",
    "ontology": "Ontology Code",
    "ontologyId": "Concept ID in the ontology",
    "relationships": [ supported relationships ],
    "counts": {},
    "dataType": {
      "name": "Data type name",
      "pattern": "Validation Regex",
      "description": "Data type description"
    },
    "attributes": {
      "visualattributes": "Visual attributes",
      "customAttributeName": "customAttributeValue"
    }
  },
  ...
]
\end{verbatim}

The \verb|dataType| field allows to know which constraints defined in the resource can be applied, thus what options can be presented to the user when a query is constructed with the help of the \verb|IRCTResource| object, see more details section~\ref{sec:designgbstep1}.
The \verb|visualattributes| field allows to modify the appearance of the concept in the UI, for example if it's a folder containing concept, or a leaf node. 
Its presence is optional so if it is absent, the appearance will stay as it is by default, this behavior is defined in the \verb|GbTreeNodesComponent| component.
The other fields can easily be mapped to original fields in Glowing Bear.

% modification of response (model and calling code)
PIC-SURE does not support the loading of more concept than the direct children, i.e. a depth of 1 is supported.
To counter this fact, we choose to load the tree at initialization time, which could potentially result in a large number of API calls depending on the size of the tree.
A potential mitigation to this would be to implement the ability for the PIC-SURE to return a complete tree, but this is left as a "nice-to-have" feature.
% todo: put it in future work

% modification relating no depth call possible
In \verb|TreeNodeService.loadTreeNodes()|, \verb|loadTreeNext()| is called to iteratively load the nodes.
\verb|loadTreeNext()| needs to be modified to account for \verb|ResourceService.getTreeNodes()| not being able to load with a depth greater than one, 
meaning it must be called for every node of the tree.
The difference between a node and a leaf is made with the use of the \verb|relationships| field: if it has a \verb|CHILD| supported relationships, it is a node for which children can be requested.
A limit to the depth at which the tree is requested is put in place, with a value that should be experimentally chosen to optimize the user experience.

% about format of node JSON
\verb|TreeNodeService.processTreeNodes()| and \verb|processTreeNode()| process the received JSON to load it into the internal \verb|treeNodes| array containing the tree in memory, they should be adapted to fit the PIC-SURE JSON format.
This allows all the other parts of Glowing Bear that use the node to access the information needed.


\subsubsection{Query Step 1: Constraints}

% UI query construction (constraints)
When adding criterion at the step 1, the UI has to know what data type is the entity in order to ask the user more information.
Here are the data types supported by Glowing Bear, and the parameters they support no it is the constrains
\begin{itemize}
    \item CONTAINS: no field
    \item 
\end{itemize}

\verb|GbConstraintComponent.onDrop()| processes the node being dropped in the query construction panel by calling \verb|ConstraintService.generateConstraintFromSelectedNode()| to generate the constraint based on the dropped node.
\verb|ConstraintService.generateConstraintFromSelectedNode()| is modified to accommodate the change from the tranSMART node types to the PIC-SURE data types.
\begin{itemize}
    \item \emph{STUDY}: removed, the study is treated as a simple node in the tree (with a dedicated visual attributes)
    \item \emph{NUMERIC}: the equivalent are all the primitive numeric PIC-SURE data types (\emph{INTEGER}, \emph{DOUBLE}, \emph{FLOAT}, \emph{DOUBLE}, \emph{BYTE})
    \item \emph{CATEGORICAL}: the equivalent is a data type that has a predefined list of permitted values
    \item \emph{DATE}: the equivalent are all the primitive date PIC-SURE data types (\emph{DATE}, \emph{DATETIME})
    \item \emph{TEXT}: the equivalent is the \emph{STRING} data type
    \item \emph{UNKNOWN}: the equivalent is no data type, the mechanism that recursively combines the sub-nodes (up to depth 6) with an \emph{OR} remains the same
\end{itemize}

From these nodes are generated the actual constraint that constitutes the query.
The \verb|ConstraintService.generateConstraintFromConstraintObject()| method originally generates the query constraint based on the constraint coming from the tree.
It is modified to take as input the data types coming from the PIC-SURE tree, and is using the current \verb|IRCTResource| object to get the constraints corresponding to the data types.
remove study, only have concepts, write for combinations (and or not) then list for values
big change in general
\begin{itemize}
    \item \emph{Concept}: maps to \emph{CONTAINS} constraint
    \item \emph{Study}: removed, it is exposed in the tree as another queryable concept, distinguished with its visual attribute for the user
    \item \emph{AND/OR} and \emph{Negation} : equivalent is the logical operators declared by resource (commonly \emph{AND/OR/NOT})
    \item \emph{Time}: 
    \begin{itemize}
        \item concept with a \emph{DATE} value: maps to nodes with data types \emph{DATE} and \emph{DATETIME}
        \item for the time dimension of the observations: TBD %TODO
    \end{itemize}
    \item \emph{Field / Trial-Visit}: TBD
    \item \emph{Value}: this depends on the type of the value, refer to the previous paragraph
    \item \emph{Patient Set}: TBD % importation of patient ids
    \item \emph{Patient subselection}: todo: how different from patient set
    \item \emph{True}: everything (not supported)
    \item \emph{Relation / Pedigree}:???
\end{itemize}

time: in order to query date in observation (dimension), and also as a value of the concept


come back to this after

value is the concept AND the constraint value always (through gb at least)
subselection is getting the patient dimension

todo: gbconstraintcomponent.updatecounts()
todo: apply trial visit constraint
todo: apply observation date constraint

most basic query: where with a PUI

select count 
-> dimension patient? what should be default? observations? 
-> no default: dimension always specified, because we want to count both patients and observation 
or the default is observation: and then dimension is specified 

MAKE USE OF DIMENSION TYPE

\verb|getCounts()| is merged in \verb|getAggregate()|: the aggregate call to PIC-SURE retrieves (if the resource support it) count, max, min, unique values and average. // concept? check now
As support for aggregate values is not mandatory for the resources, if the resource does not support it the related features are disabled (more info in section~\ref{sec:featuresdisable}.

originally how GB is checking: it is getting the (<ConceptConstraint>this.constraint).concept.type: this is the object created from the concepts tree
then the html displays the div according to the concept type: does this by matching against the resource information ]
put in the details on how to handle the values and all

there should be an auto match, between code and UI stuff , and the type of data
example: if it has permitted values it is categorical
if it has some primitive fields the type can be matched against it
string as well

GOAL: construct the where query, with the proper predicate (we get it from the drag n drop)


The API request is:
\begin{verbatim}
POST /rest/queryService/runQuery
{
    clauseId: 
}
\end{verbatim}

Response:
\begin{verbatim}
[
  {
    "pui": "/<resource>/<path>/",
    "name": "Concept internal name",
    "displayName": "Concept name",
    "description": "Concept description",
    "ontology": "Ontology Code",
    "ontologyId": "Concept ID in the ontology",
    "relationships": [],
    "counts": {},
    "dataType": {
      "name": "Data type name",
      "pattern": "Validation Regex",
      "description": "Data type description"
    },
    "attributes": {
      "visualattributes": "Visual attributes",
      "customAttributeName": "customAttributeValue"
    }
  },
  ...
]
\end{verbatim}


the core: query

Example of PIC-SURE API query:

whereclause:
contains field, field contains pui and datatype, pui allows to creates entity
predicate is gotten
clausefields: map: path -> field

\begin{verbatim}
{
  "where": [ {
    "field": {
      "pui": "/nhanes/Demo/laboratory/laboratory/biochemistry/Creatinine, urine (mg per dL)/", 
      "dataType": "STRING"
    },
    "predicate": "CONTAINS"
    } ],
  "select": [ {
    "field": {
      "pui": "/nhanes/Demo/laboratory/laboratory/biochemistry/Creatinine, urine (mg per dL)/",
      "dataType": "STRING"
    },
  "alias": "Creatinine%2C%20urine%20%28mg%20per%20dL%29"
  } ]
}
\end{verbatim}

same principle for predicates: GB will support some but not all 

from transmart api: OK to use concept path
 "type": "concept",
                "path": "\\Public Studies\\EHR\\Diagnosis\\",

PREDICATE MANDATORY in query
-> CONTAINS predicate: simple exists ( no field ) --> this is the default predicate (put that in minimum stuff) or not necessarly? figure that out!



1 where clause + 1 predicate
multiple predicate: multiple where clause (linked with logical operator: how?)

QUERY UI:
when drag n dropping concept, we know what data type it is, we know which constrains can be applied on it, what is their name, validation regex, to construct the where constraint (like now)

also we 

DATA type is needed in query

select operation: for the aggregation

query object in QueryService: modif model to fit PIC SURE queries

ConstraintService.generateSelectionConstraint() : generate constraint with inclusion / exclusion: rename to generateConstraint


from constraint: getTreeNodeDescendantsWithExcludedTypes / getConceptFromTreeNode / 
from query service : 
- updateProjectionTreeData : tree table? seems like it s for selectingv data in step 2 // shouldnt be modified normally
- updateTreeNodeCounts: update the counts with query results: shouldnt need modif
gb-projection component?

\subsubsection{Saving a Query}
Only the API calls to handle query saving need to be modified, the calling code remains the same.
The IRCT implementation to save and re-use saved queries is not complete: it is originally possible to only save queries, but not list or load them. 
See section~\ref{sec:irctsavedqueries} for the additional implementation in IRCT to add these features.

% saveQuery
In \verb|ResourceService.saveQuery()| the API request becomes:
\begin{verbatim}
POST /rest/queryService/saveQuery
{
    "queryName": <query_name>,
    "query": {
        <query_body>
    }
}    
\end{verbatim}

Response:
\begin{verbatim}
{
    "queryId": <query_id>
}    
\end{verbatim}

% getQueries
In  \verb|ResourceService.getQueries()| the API request becomes:
\begin{verbatim}
GET /rest/queryService/queries
\end{verbatim}

Response:
\begin{verbatim}
{
    [
        "queryId": <query_id>,
        "queryName": <query_name>,
        "query": {
            <query_body>
        }
    ], [
        ...
    ],
    ...
}    
\end{verbatim}

% updateQuery
In  \verb|ResourceService.updateQuery()| the API request becomes:
\begin{verbatim}
PUT /rest/queryService/queries/<query_id>
{
    "queryName": <query_name>,
    "query": {
        <query_body>
    }
}
\end{verbatim}

Response:
\begin{verbatim}
{
    "message": <status_message>
} 
\end{verbatim}

% deleteQuery
In  \verb|ResourceService.deleteQuery()| the API request becomes:
\begin{verbatim}
DELETE /rest/queryService/queries/<query_id>
\end{verbatim}

Response:
\begin{verbatim}
{
    "message": <status_message>
} 
\end{verbatim}


\subsubsection{Query Step 2:  Data Selection}

In the PIC-SURE paradigm, this step is about preparing the \emph{select} statement of the query: the output for step 3 is the added \emph{select} causes to the query.


%Component orchestrating the step 2 is originally the \verb|GbProjectionComponent|

QueryService.updateCounts_2


what do we have form step 1? the counts (osef), 



\subsubsection{Query Step 3:  Data Export}





%Component orchestrating the step 3 is originally the \verb|GbExportComponent|


GET /resultService/available: returns the list of available results (of the user)
returns the result id and the status
<<< GB does first the call to get job list

GET /resultService/resultStatus/ID
if problem: message field (always try to get it, but can be null)
if OK, fields are: result id, status, startime, endtime, data type

GET /resultService/availableFormats/ID
returns list of formats for a specific result

GET /resultService/result/ID/format?download=yes
download can be put to no (content type will change, set yes for download from browser with clickable link)


some formats may not be available, but could be implemented: do match and check with ward/ bo which to support

--> match of the columns? possible to cancel in irct?NO, status returned ? YES

GB original: 
/file\_formats
/data\_formats


\subsection{IRCT Resources Implementation}
some to be modified, some to be implemented

- add support for shrine/medco with additional JAR packaged and put in wildfly
- add systems (by type) with pq-psql functions

resources modficiatons: add the agreggates (i2b2 just count, avg/min/max seems to be complicated? check it out)
i2b2: no select (currently), because it is mainly about the constraints: where clauses

importatn to specify what is done with minimal set of things supported

--> include datatype in query: saves a req to tree node
\paragraph{i2b2}

\paragraph{SHRINE}

\paragraph{tranSMART 16.2}

\paragraph{tranSMART 17.1}
last step is transmart 17.1 as connector, because it is if time allows


%\subsection{Current Status of Softwares}
%\subsection{Glowing Bear}
%desc of how it works
resource info in db -> example on how to do it with i2b2
through the api resourceService/resources get us all the resources and their capabilities(by type, but we want query for now i guess?)
define:
- predicates
- relationships
- the entities also define relationships, what is it exactly?
-> for GB we would need a minimum set of capabilities??

-> study should become just a parameter of the tree entity, but we shouldnt care about it
-> how / at which stage implement that in Gb?

edu.harvard.hms.dbmi.bd2k.irct.cl.rest;
classes implementing the REST API, using javax.ws.rs / JAX-RS: Java API for RESTful Web Services

---

-- irct --
version from github to select: which? there seem to be a new query param (commit on feb 2) only\_count, which may prove valuable for GB << important question to solve, check out a specific commit? last release is not recent enough as of now 

--> in the queryesrvice, when running query, there is a parameter "only\_count" that exists (in the later version)
it is added to the metadata of the query, to be processed by the RI (i guess support not mandatory) 
-> to have less processing, use it from GB!

select aggregate (count), they exist, check out: https://github.com/hms-dbmi/IRCT/blob/ae31f79ef2015bc6ae349e30e2a6652a22141b07/IRCT-RI/src/main/resources/SciDB.sql

aggregate is a type of select, and count is a type of aggregate, totally OK to do that 



--Resources--
does a lib in java to talk to transmart rest v2 exists? would be nice


todo: transmart cache the ontology because depth can be specified

implekmentaion of transmart: study as first layer? no just the tree presentede by transmart

define interface for java / sql skeleton for GlowingBearFeatureName (counts, etc.)


RI transmart: thourhg the path, if within study: study is queried, if cross-study: no study queried, basically do as the constraint is specifying
+ to find the study associated to a concept, simply go up, or keep in cache a list of studies
Moreover the tree concept of tranSMART has at its root the studies
expose in tree as DIMENSION or as STUDY? --> dimension (like this lcinical trial also supported)


%NOTES TO SORT%%%%%%%%%%%%%%%%%%%%%%
resource.service.ts -> this is what implements the api
study -> only use looks to be to restrict during query
then it can simply be another dimension of the data, as a constraint, but it is also in the tree, how to do that?
--> dimension exposed in the tree sounds good

subsec for mapping API calls
swubsec on how to implement change in API call: genericsize the calls, get the enabled/disabled features (methods to determine if enabled or not hardcoded), then change the actual things

---
at init: see what features is supported by the resource the user choose to use
get a list of features that are OK / not OK -- and TODO: add a step of implementing the disabling of some feature
sth else: seems quite a bit is studies based, this should be abstracted into the resource implementation

calls:
--- init step 0
1. /studies: list of studies, displayed under "Public Studies", some under "Private Studies" (distinction how??), and more, but how exactly are they mapped to the tree?

2. --> they are mapped with call to /tree\_nodes?root=\\ => each of the concept has a studyId to do the mapping
--> call to /tree\_nodes has also parameters depth --> IRCT only has depth 1, tree traversal in UI might need modifs

3. /queries: list of saved queries returned

4. /jobs: export jobs (running or not) --> not sure how IRCT handles that -> investigate / might need a change

5. /counts\_per\_study\_and\_concept: return counts for all studies / sub things -> see if how to map to irct, either as data or as process?

6. /counts: with param true, to display the total counts available

--- query step 1
7. /aggregates\_per\_concept : metadata of the concept, to display min max count etc. (aggregate data)
triggered on drag n drop of concept
--> use aggregate just as scidb again (so this should be a requirement of GB-or if not feature disabled)

POST /v2/observations/aggregates\_per\_concept
{constraint: {type: "concept", conceptCode: "O1KP:AGE"}}

{
  "aggregatesPerConcept": {
    "O1KP:AGE": {
      "numericalValueAggregates": {
        "avg": 51.25,
        "count": 1200,
        "max": 65.0,
        "min": 50.0,
        "stdDev": 4.147509477069983
      }
    }
  }
}

--> select aggregate
-> different category of feature enabled disabled is the min/max (because of i2b2) -> this should be quite fine grained, maybe directly associated to the specific function exposed by the resource

SELECT COUNT should be 1 call: no second call to get data


8. /elements: param contraint: which concept // not sure of what the answer if for?
GET /v2/dimensions/trial\%20visit/elements
constraint:{"type":"concept","conceptCode":"O1KP:AGE"}

{
  "elements": 
  [
    {
      "id": -101,
      "relTimeLabel": "General",
      "relTimeUnit": null,
      "relTime": null
    }
  ]
}


starting here possible to change constraints & concepts in UI, then click on update counts:
9. /counts\_per\_study\_and\_concept: update the counts according to the selected concepts-> return all relevant count (to update the tree) // constraint as param
POST /v2/observations/counts\_per\_study\_and\_concept
{
  "constraint": {
    "type": "subselection",
    "dimension": "patient",
    "constraint": {
      "type": "and",
      "args": [
        {
          "type": "and",
          "args": [
            {
              "type": "concept",
              "conceptCode": "O1KP:AGE"
            },
            {
              "type": "value",
              "valueType": "NUMERIC",
              "operator": ">=",
              "value": 55
            }
          ]
        },
        {
          "type": "study_name",
          "studyId": "ORACLE_1000_PATIENT"
        }
      ]
    }
  }
}

{
  "countsPerStudy": {
    "ORACLE_1000_PATIENT": {
      "O1KP:AGE": {
        "observationCount": 100,
        "patientCount": 100
      },
      "O1KP:CAT1": {
        "observationCount": 100,
        "patientCount": 100
      },
      "O1KP:CAT10": {
        "observationCount": 100,
        "patientCount": 100
      },
      ......
    }
  }
}

--> this would require a groupb by count basically? maybe that can be passed as a parameter of the select query
that's where to get this feature enabled, resource needs to have aggregate w/ count, and also another select parameter which for which concepts you want the counts

there is both counts per study, and counts per study and concepts / merge into 1 thing (study becomes a dimension)
implementation STEP study=dimension

so here, we use aggregate count, param the list of dimensions we wanna query-analog to modifier

/counts



---
runQuery: use POST /runQuery?only\_count\&full\_response to get with 1 call the result
--- query step 2: about selecting data  / fields we wanna get from the selected patients / observations 


todo: put examples of how a transmart rest v2 query would be using irct, for all of them!

% todo: in irct modifs: add query name to the saved queries / add getQueries that returns the whole queries / add getQuery/id with PUT, DELETE, methods



