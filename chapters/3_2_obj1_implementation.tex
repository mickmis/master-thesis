\section{Implementation of the Interoperability Layer}
Here are described step-by-step the implementation of the solution presented in the last section. 
Each of the step first explains what is the current status of the relevant components, and then what needs to be modified or implemented.
roadmap todo
%nature of the thing-> should be test-driven


first target: most basic form, tree used to make queries


\subsection{Deploying Original Components}
First we are deploying an instance of IRCT, with an instance of i2b2 and tranSMART 16.2 which are both natively supported. 

-> setup of the individual building blocks

gb
transmart 17.1

data loaded

how is it done etc.

1. irct setup + i2b2 + transmart 16.2 + shrine
deployment mainly, data? i2b2 demo data? fortransmart? 
DB setup for the systems
impl for shrine maybe? (extend the i2b2 one)

details of in appendix~\ref{sec:docker-images}


\subsection{Glowing Bear Modifications}

% GB modif overview + why no transmart rest v2
Glowing Bear is pretty heavily modified, the idea being to make it a native PIC-SURE API client by replacing completely the tranSMART client API implementation. 
Compatibility with tranSMART versions 17.1+ is later restored through the implementation of an IRCT resource interface (see~\ref{subsec:todo}).
The reasoning behind this choice is that maintaining compatibility of two different but similar APIs in Glowing Bear would be possible, but complicated, which translates into additional efforts spent on the implementation, and later on the maintenance of the code: this would be sub-optimal as these efforts are better spent elsewhere.
The potential downside of this choice is a time delay for the requests as we are introducing an additional middle-component, these will be formally measured in chapter~\ref{chap:perfeval}.

% overview of steps taken in GB workflow: steps for modifications
After reviewing what features will be removed from Glowing Bear, the organization of this section follows its typical workflow:
\begin{itemize}
    \item Client initialization: loading and login
    \item Explore the tree of concepts
    \item Step 1: Construct a query or re-use a saved query
    \item Save a constructed query
    \item Step 2: Select the data to export
    \item Step 3: Export the data
\end{itemize}

% todo: check, integrate or not
% PIC SURE vocabulary: entity / concept
% Here we will use the PIC-SURE vocabulary.
% Implementation: change the names (concept -> entity)
% Changes in Glowing Bear take root in the modification of the API calls, that are located in \verb|ResourceService|, this is thus what we take as starting points in this section. 
% STEP 1: defining the where clauses (and a few count queries)
% STEP 2: defining the SELECT clauses (what data do we want)
%Request: modify in resource service
%response: modify in models

% todo: map step 1=where, step 2= select, step 3=export (data tables new step 3, where?) // 



% --- text of previous version:
%The study can however be considered as a dimension of the observations, something that can be supported generically through PIC-SURE.
%Which means that this transition of studies to the more generic concept of dimension should also be applied in Glowing Bear, i.e. the behavior that is study specific should disappear.
%We rename the module \verb|gb-study-constraint| to \verb|gb-dimension-constraint| to handle constraints based on dimension, if the resource exposes such entities in its tree.
%In order to keep the name auto-completion feature, the list of possible values for the queried dimension is populated wiht the following query to PIC-SURE:
% \begin{verbatim}
% {
%   "select": [ {
%     "field": {
%       "pui": "/<resource>/<path_to_dimension_entity>",
%       "dataType": "DIMENSION"
%     },
%   "alias": "Creatinine%2C%20urine%20%28mg%20per%20dL%29"
%   } ]
% }
% \end{verbatim}


\subsubsection{Authentication \& Authorization}

The modification to Glowing Bear is the migration from the authorization protocol OAuth2~\cite{oauth2} to the authentication and authorization protocol OpenID Connect~\cite{openidconnect}.
IRCT uses OpenID Connect to authenticate and authorize the users of its client applications, while Glowing Bear originally implements the client side of the OAuth2 protocol for authorization with tranSMART. 
Since OpenID Connect is a layer on top of OAuth2, the modifications to migrate the client code from OAuth2 to OpenID Connect are not significant, and are made by making use of the \emph{angular-auth-oidc-client} library~\cite{angular-auth-oidc-client}.
The flow remains the same: Glowing Bear obtains a JSON Web Token (JWT) from the OpenID Connect provider, which will then be embedded in the header of the HTTP requests to IRCT.

\subsubsection{IRCT Resources Support}

\paragraph{Resources List}
After the user has successfully logged in, Glowing Bear requests the list of IRCT resources available and their properties.
Then the user chooses from the list which resource to use for the current session: it is possible to use only one resource at a time.
The selected resource and its properties are kept in the new \verb|IRCTResourceService| service that keeps and processes all the information about the resource returned by PIC-SURE.
The method \verb|ResourceService.getResources()| is added to retrieve the PIC-SURE resources with the following API call:
\begin{verbatim}
GET /rest/resourceService/resources
\end{verbatim}

Response:
\begin{verbatim}
[
  {
    "id": 1,
    "name": "resource name (e.g. i2b2-local)",
    "implementation": "resource type (which implementation is used, e.g. i2b2XML)",
    "relationships": [ supported relationships between tree nodes, e.g. CHILD ],
    "logicaloperators": [ "AND", "OR", "NOT" ],
    "predicates": [ supported predicates and their properties, e.g.: {
        "predicateName": "predicate name (e.g. CONTAINS)",
        "displayName": "displayed name (e.g. Contains)",
        "description": "description",
        "default": true if this predicate should be selected by default,
        "fields": [ {
            "name": "field name (e.g. By Encounter)",
            "path": "field code (to be used in query, e.g. ENCOUNTER)",
            "description": "By Encounter",
            "required": true,
            "dataTypes": [ data type of the value field(s) of this predicate ],
            "permittedValues": [ permitted values, if it categorical ]
          } ],
        "dataTypes": [ data types to which this predicate applies (e.g. STRING) ],
        "paths": [ paths to which this predicate applies (empty for all) ]
      } ],
    "selectOperations": [ supported operations for select (e.g. AGGREGATE) ],
    "selectFields": [ supported fields for the operations, e.g. COUNT for AGGREGATE ],
    "joins": [ ],
    "sorts": [ ],
    "processes": [ ],
    "visualization": [ ],
    "dataTypes": [ data types and their properties, e.g.: {
        "name": "name of the data type",
        "pattern": "regex to validate the value",
        "description": "description of the data type"
    } ]
  },
  ...
]
\end{verbatim}


\paragraph{Specific Features}
\verb|IRCTResourceService| provides methods that determine if some specific features are supported, which allows some features of Glowing Bear to be enabled or disabled:
\begin{itemize}
    \item \verb|supportsCounts()|: returns \emph{true} if queries of type \verb|SELECT COUNT| are supported, which enables the live counts display in the UI
    \item \verb|supportedCountsDimensions()|: if \verb|supportsCounts()| is \emph{true}, returns the countable dimensions (most commonly \emph{Patients} and \emph{Observations}).
    \item \verb|supportsNestedClauses()|: returns \emph{true} if the \emph{where} clause predicates \verb|CLAUSE_NEST| and \verb|CLAUSE_UNNEST| allowing nesting are supported
    \item \verb|| supports: min, max (others needed for GB?)
    \item \verb|supportsStudies()|: returns \emph{true} is queries of type \verb|SELECT STUDIES| are supporter
\end{itemize}

% todo: add for the data types capabilities, what does it return exactly? predicate?

\subparagraph{Studies}

% general + into tree
The tranSMART REST API v2 has several calls that are specific to studies, while the PIC-SURE API does not have a direct equivalent to this.
However since the studies are completely embedded within the concept tree, i.e. a concept belongs to one study or it is cross-study, they can simply be abstracted into the tree of entities exposed by PIC-SURE: we create a \verb|STUDY| data type for the tree entities (see~\ref{sec:gb-tree}).
\verb|TreeNodeService.isTreeNodeAStudy()| implements the recognition of the \verb|STUDY| data type.

% list of studies
If the resource supports it, \verb|ConstraintService.loadStudies()| loads the list of studies by calling \verb|ResourceService.getStudies()|, which is modified to use the following PIC-SURE call:
\begin{verbatim}
POST /rest/queryService/runQuery?full_response
{
  "select": [ {
    "operation": "STUDIES"
  } ],
  "alias": "get_studies"
}
\end{verbatim}



\subsubsection{Concepts Tree}
\label{sec:gb-tree}
%todo: check consistency for level of details

% modification of request (api call)
The API call in \verb|ResourceService.getTreeNodes()| is modified according to the following:
\begin{itemize}
    \item Parameters
    \begin{itemize}
        \item \verb|depth| is removed: PIC-SURE always return a depth of 1
        \item \verb|hasCounts| is removed: PIC-SURE always return a counts field (filling it or not depend on the resource implementation); additionnally in the current implementation it is never used
        \item \verb|hasTags| is removed: PIC-SURE always return nodes metadata (content is resource dependent)
        \item \verb|relationship| is added: tree browsing is generic in PIC-SURE, relationships between nodes are defined by resource, \verb|child| and \verb|parent| at least are supported
    \end{itemize}
    \item 
\end{itemize}

The API request becomes:
\begin{verbatim}
GET /rest/resourceService/path/<resource>/<path>/?relationship=<relationship>
\end{verbatim}

Response:
\begin{verbatim}
[
  {
    "pui": "/<resource>/<path>/",
    "name": "Concept internal name",
    "displayName": "Concept name",
    "description": "Concept description",
    "ontology": "Ontology Code",
    "ontologyId": "Concept ID in the ontology",
    "relationships": [ supported relationships ],
    "counts": {},
    "dataType": {
      "name": "Data type name",
      "pattern": "Validation Regex",
      "description": "Data type description"
    },
    "attributes": {
      "visualattributes": "Visual attributes",
      "customAttributeName": "customAttributeValue"
    }
  },
  ...
]
\end{verbatim}

The \verb|dataType| field allows to know which constraints defined in the resource can be applied, thus what options can be presented to the user when a query is constructed with the help of the \verb|IRCTResourceService|, see more details section~\ref{sec:designgbstep1}.
The \verb|visualattributes| field allows to modify the appearance of the concept in the UI, for example if it's a folder containing concept, or a leaf node. 
Its presence is optional so if it is absent, the appearance will stay as it is by default, this behavior is defined in the \verb|GbTreeNodesComponent| component.
The other fields can easily be mapped to original fields in Glowing Bear.

% modification of response (model and calling code)
PIC-SURE does not support the loading of more concept than the direct children, i.e. a depth of 1 is supported.
To counter this fact, we choose to load the tree at initialization time, which could potentially result in a large number of API calls depending on the size of the tree.
A potential mitigation to this would be to implement the ability for the PIC-SURE to return a complete tree, but this is left as a "nice-to-have" feature.
% todo: put it in future work

% modification relating no depth call possible
In \verb|TreeNodeService.loadTreeNodes()|, \verb|loadTreeNext()| is called to iteratively load the nodes.
\verb|loadTreeNext()| needs to be modified to account for \verb|ResourceService.getTreeNodes()| not being able to load with a depth greater than one, meaning it must be called for every node of the tree.
The difference between a node and a leaf is made with the use of the \verb|relationships| field: if it has a \verb|CHILD| supported relationships, it is a node for which children can be requested.
A limit to the depth at which the tree is requested is put in place, with a value that should be experimentally chosen to optimize the user experience.

% about format of node JSON
\verb|TreeNodeService.processTreeNodes()| and \verb|processTreeNode()| process the received JSON to load it into the internal \verb|treeNodes| array containing the tree in memory, they should be adapted to fit the PIC-SURE JSON format.
This allows all the other parts of Glowing Bear that use the node to access the information needed.


\subsubsection{Query Step 1: Constraints}

% GB overall workflow for queries
The original Glowing Bear workflow for creating the constraints is the following: \\
\verb|GbConstraintComponent.onDrop()| processes the node being dropped in the query construction panel in the UI by calling \verb|ConstraintService.generateConstraintFromSelectedNode()| to generate the constraint based on the dropped node. \\
It uses \verb|ConstraintService.generateConstraintFromConstraintObject()| to construct the individual constraint objects.

The constraints generated in the step 1 of the query corresponds to the \emph{where} clauses of the PIC-SURE query: they define the criterion the resulting data must satisfy.
The components based on \verb|modules/gb-data-selection-module/constraint-component/GbConstraintComponent| and the models based on \verb|models/constraint-models/Constraint| correspond to the different PIC-SURE predicates that Glowing Bear supports.
Overall the Glowing Bear workflow stays the same at a high-level, but its implementation at the low-level undergoes significant changes to bring compatibility with PIC-SURE.

\paragraph{Constraints Logic}
\label{sec:gb-predicates}

\verb|ConstraintService| is initialized with \verb|IRCTResourceService|: this allows the service to link the data types of the tree nodes with the constraints they support. \\
\verb|ConstraintService.generateConstraintFromConstraintObject()| is the core method of the constraint generation, and is thus completely re-implemented and is renamed \verb|generateConstraintFromDataType()|.
It is modified to take as input the data types coming from the PIC-SURE tree, and is using the \verb|IRCTResourceService| to get the constraints corresponding to the data types, returning a \verb|Constraint| object.
\verb|generateConstraintFromSelectedNode()| is modified to have two cases only: it is a queryable node, i.e. it has a data type, or not. If it has a data type it uses \verb|generateConstraintFromDataType()| to get the constraint, if not it calls itself recursively with the children nodes.
\verb|generateConstraintFromDataType()| supports the following PIC-SURE data types:
\begin{itemize}
\item Primitive
    \begin{itemize}
        \item Numeric types: \verb|integer|, \verb|long|, \verb|float|, \verb|double|
        \item Date types: \verb|date|, \verb|dateTime|
        \item String type: \verb|string|
    \end{itemize}

\item Custom
    \begin{itemize}
        \item Enumerated type: \verb|enum_value| (enumerated value exposed through the tree and not as a value)
        \item Ontology concept type: \verb|concept| (simple concept without value)
        \item Study: \verb|study| (restrict to a specific study)
    \end{itemize}
\end{itemize}

\subparagraph{\emph{where} Models}

% constraint models
The way the constraints are represented internally need modification, as the nature of the tranSMART constraints and the PIC-SURE \emph{where} clauses are slightly different: they are more generic, but more importantly the supported predicate for each data type are known only at the runtime.
Constraints models in \verb|src/app/models/constraint-models/| are now not based on the type of the constraint, but on the predicate used for the constraint.
The interface \verb|Constraint| remains, but the members \verb|toQueryObjectWithSubselection()|, \verb|toQueryObjectWithoutSubselection()| and \verb|parent| are removed.
The equivalent of the subselection in PIC-SURE would be the dimension, but 
\begin{enumerate*}[label=(\arabic*)]
  \item it is defined by the resources themselves,
  \item it is integrated into the \emph{select} clauses, which is handled in the second step as it is not considered a constraint;
\end{enumerate*}
justifying the removal of those members.
They also now contain data about themselves specified by the IRCT resource: predicates name, description, paths and data types it applies to, its fields (name, code, description, required flag, data type and permitted values).

% constraint models JSON generation example
\verb|toQueryObject()| takes care of generating the JSON of the constraint: all the implementing methods now needs to generate the PIC-SURE \emph{where} clauses.
Example of two different \emph{where} clauses that would be produced by the corresponding \verb|toQueryObject()| methods:
\begin{verbatim}
{
    "field": {
        "pui": "/transmart/study1/Gender/Male/",
        "dataType": "ENUM_VALUE"
    },
    "predicate": "CONTAINS"
}

{
    "field": {
        "pui": "/transmart/study1/Age/",
        "dataType": "INTEGER"
    },
    "predicate": "CONSTRAIN_VALUE_NUMERIC",
    "fields": {
        "OPERATOR": "==",
        "CONSTRAINT": "5"
    }
}
\end{verbatim}

% constraint models factory
\subparagraph{Instantiating Constraints}
Because the predicates and the fields they have are known only at runtime, we create a service \verb|ConstraintFactoryService| that handles the instantiating of properly initialized \verb|Constraint| objects.
A method \verb|createConstraint()| taking as parameters the data type and and predicate returns the \verb|Constraint| object.
While some pre-defined predicates are supported by Glowing Bear (see list \ref{todo}), or even required for some features, not all can be supported. 
For this reason a generic \verb|Constraint| is created, that allows Glowing Bear to handle unknown constraints that resources might declare.

% list of types of constraints: PREDICATE
The difference constraint models with their associated predicate, that together support all the PIC-SURE data types listed in~\ref{sec:gb-predicates}, are listed below:
\begin{itemize}
    \item \verb|ContainsConstraint|: predicate \verb|CONTAINS|, valid for \verb|enum_value|, \verb|study|, \verb|concept| 
    \item \verb|NumericValueConstraint|: predicate \verb|CONSTRAIN_VALUE_NUMERIC|
    \item \verb|StringValueConstraint|: predicate \verb|CONSTRAIN_VALUE_STRING|
    \item \verb|DateValueConstraint|: predicate \verb|CONSTRAIN_VALUE_DATE| (when a concept has a date value)
    \item \verb|DateConstraint|: predicate \verb|CONSTRAINT_DATE| (when temporal data is filtered by date, e.g. the obervation date)
    % todo: patient set? clinical-trial visit? relation/pedigree?
\end{itemize}


\paragraph{Logical Operators}
Glowing Bear supports the definition of nested inclusion criteria, with the criteria belonging to the same group being linked by a logical operator \emph{AND} or \emph{OR}.
PIC-SURE allows queries with several \emph{where} clauses and lets each resource declares the logical operators it supports to link them. 
However the link between the clauses is flat, a clause defines its relationship with the previous clause: nested queries are not possible natively with PIC-SURE, but a workaround is presented below.

For resources that support nested queries, they declare the support for the \verb|CLAUSE_NEST| and \verb|CLAUSE_UNNEST| predicates for \emph{where} clauses.
By using these it is possible for resources to support nested queries, see the following example for the methodology:
Consider the following nested query constructed in Glowing Bear:
\begin{verbatim}
    ((H AND B) OR (D AND S)) AND X AND (H OR F)
\end{verbatim}
It would have the PIC-SURE query with the following \emph{where} clauses:
\begin{verbatim}
{
    "where": [
        {"predicate": "CLAUSE_NEST" },
        {"predicate": "CLAUSE_NEST" },
        { H },
        { B, "logicalOperator": "AND" },
        {"predicate": "CLAUSE_UNNEST" },
        {"predicate": "CLAUSE_NEST", "logicalOperator": "OR" },
        { D },
        { S, "logicalOperator": "AND" },
        {"predicate": "CLAUSE_UNNEST" },
        {"predicate": "CLAUSE_UNNEST" },
        { X, "logicalOperator": "AND" },
        {"predicate": "CLAUSE_NEST", "logicalOperator": "AND" },
        { H },
        { F, "logicalOperator": "AND" },
        {"predicate": "CLAUSE_UNNEST" }
    ]
}
\end{verbatim}
+
To support this construction, a model \verb|PICSUREWhereAttribute| is created, it contains an array of \verb|Constraint| and appropriately sets their \verb|logicalOperator| fields and adds the nesting \emph{where} clauses.
\verb|CombinationConstraint| and \verb|GbConstraintComponent| are modified so that they disallow nested queries if the resource does not support it.
This is reflected in the UI by removing the \verb|add criterion| boxes for the attributes with a level equal to or lower than 1.
\verb|ConstraintService.generateConstraintFromConstraintObject()| is modified so that the (un)nesting \emph{where} clauses are generated when the resource supports it.

% todo: what about the NOT???? made with exlucsion constraint, sep query and substraction: check it out 

% ---


\subparagraph{Generating \emph{where} Attribute}
The method \verb|ConstraintService.generateSelectionConstraint()| is what is used by other parts of the code to generate the constraints defined in the first step in a format that fits the API call.
We rename it to \verb|generateWhereAttribute()| to fit the PIC-SURE jargon, and modify the method accordingly.
It returns a \verb|PICSUREWhereAttribute| as described in the previous paragraph.

\paragraph{User Interface}
When adding criterion at the step 1, the UI has to know what data type is the entity in order to know what input from the user is expected.
This behavior is implemented in the \verb|GbConstraintComponent| and its extending components.
These components are modified or removed to fit all the modifications previously described, to arrive at a state where there is one component for each of the supported predicates described section~\ref{todo}.
Additionally they use the information provided by \verb|IRCTResourceService| to:
\begin{itemize}
    \item offer to the user a list of the supported predicates to choose from,
    \item know what fields the predicate needs,
    \item enforce format of the fields input values with the regex or offer a dropdown list of permitted values, 
    \item enforce required or optional fields,
    \item display aggregates about the concept (such as the min, max, etc.).
\end{itemize}

The parent \verb|GbConstraintComponent| is modified to hold the data type of the dropped node, and allow for the switch between predicates (if multiple predicates are supported by the data type).
Then the extending components, one for each predicate (with the addition of the one for unknown predicates), are used according to the chosen predicate.

% todo: apply trial visit constraint (field)
% todo: apply observation date constraint
% todo: import patient sets
% todo: mention modifiers (but expose through tree)
% todo: optimization : \verb|ConstraintService.optimizeContraintObject()|
% todo: field: what field to query / dimension 
% todo: list num. values possible operators
% todo: relation/pedigree
% todo: root constraint should it be mentionned? yes there will be no root, but an array, use the model for the and/or to hold it  
% todo: rename models Constraint -> WhereClause
% todo: create model SelectClause (for count example: fields pui, dataType, Function, dimension)

\subsubsection{Query Step 1: Aggregates}

Glowing Bear does two kinds of aggregate queries: counts and min/max.
Count queries are made when the user presses \emph{Update Counts} after modifying constraints (in step 1) or after modifying selected attributes for export (in step 2).
The API calls and calling methods made are modified to use PIC-SURE as described below.

%todo: \paragraph{Counts}
\verb|ResourceService.getCounts()| is merged into \verb|getAggregate()|, which itself gets added an argument \verb|aggregateType| specifying what types of aggregate are requested, optionally a \verb|dimension|, and path. %todo: revisit, need some kind of structure to construct aggregate query
As support for aggregate \emph{select} calls is not mandatory for the resources, if the resource does not support it the related features are disabled (more info in section~\ref{sec:featuresdisable}).

An API request for a count, min or max, with the \emph{where} constraint defined in step 1, would be:
\begin{verbatim}
POST /rest/queryService/runQuery?full_response
{
  "select": [ {
    "field": {
      "pui": "/path/to/concept/",
      "dataType": "STRING"
    },
    "operation": "AGGREGATE",
    "fields": {
        "FUNCTION": "count",
        "DIMENSION": "patient"
    }
  }, {
    "operation": "AGGREGATE",
    "fields": {
        "FUNCTION": "count",
        "DIMENSION": "observation"
    }
  }, {
  "field": {
      "pui": "/path/to/concept/",
      "dataType": "INTEGER"
    },
    "operation": "AGGREGATE",
    "fields": {
        "FUNCTION": "min"
    }
  }, {
    "field": {
      "pui": "/path/to/concept/",
      "dataType": "INTEGER"
    },
    "operation": "AGGREGATE",
    "fields": {
        "FUNCTION": "max"
    }
  } ],
  "where": [ <constraints> ],
  "alias": "<query_alias>"
}
\end{verbatim}

% todo reponse
% Response:
% \begin{verbatim}
%     todo
% \end{verbatim}

Some modifications are made for code calling the API methods:
\begin{itemize}
    % getAggregate()
    \item \verb|GbConceptConstraintComponent.initializeConstraints()| is modified to adapt to the new format of \verb|ResourceService.getAggregate()| to get the \emph{min} and \emph{max}, and to get rid of the call to get categorical, as they are now exposed through the tree as explained section~\ref{sec:todo}.

    % getCounts()
    \item \verb|QueryService.updateInclusionCounts()|, \verb|updateExclusionCounts()| and \verb|updateCounts_2()| are modified to now call \verb|ResourceService.getAggregate()| to get the counts.

    % getCountsPerStudyAndConcept()
    \item \verb|QueryService.updateConceptsAndStudiesForSubjectSet()| is modified to make use of \\
    \verb|getAggregate()| to get the counts per concepts.
\end{itemize}


% TODO:
%     \item \verb|ResourceService.getCountsPerStudyAndConcept()| (replaced by getCounts())
%     \item \verb|ResourceService.getCountsPerStudy()|

% Note that the use of \verb|getCountsPerStudyAndConcept()| and \verb|getCountsPerStudy()| is replaced by the more generic \verb|ResourceService.getCounts()|.
% \verb|QueryService.updateConceptsAndStudiesForSubjectSet()| and \verb|updateConceptsAndStudies()| are removed reference to and use of studies.


\subsubsection{Saving a Query}
On top of modifying the \verb|Query| model to fit the PIC-SURE format, only the API calls to handle query saving need to be modified, the calling code remains the same.
The IRCT implementation to save and re-use saved queries is not complete: it is originally possible to only save queries, but not list or load them. 
See section~\ref{sec:irctsavedqueries} for the additional implementation in IRCT to add these features.

% saveQuery
In \verb|ResourceService.saveQuery()| the API request becomes:
\begin{verbatim}
POST /rest/queryService/saveQuery
{
    "queryName": <query_name>,
    "query": {
        <query_body>
    }
}    
\end{verbatim}

Response:
\begin{verbatim}
{
    "queryId": <query_id>
}    
\end{verbatim}

% getQueries
In  \verb|ResourceService.getQueries()| the API request becomes:
\begin{verbatim}
GET /rest/queryService/queries
\end{verbatim}

Response:
\begin{verbatim}
{
    [
        "queryId": <query_id>,
        "queryName": <query_name>,
        "query": {
            <query_body>
        }
    ], [
        ...
    ],
    ...
}    
\end{verbatim}

% updateQuery
In  \verb|ResourceService.updateQuery()| the API request becomes:
\begin{verbatim}
PUT /rest/queryService/queries/<query_id>
{
    "queryName": <query_name>,
    "query": {
        <query_body>
    }
}
\end{verbatim}

Response:
\begin{verbatim}
{
    "message": <status_message>
} 
\end{verbatim}

% deleteQuery
In  \verb|ResourceService.deleteQuery()| the API request becomes:
\begin{verbatim}
DELETE /rest/queryService/queries/<query_id>
\end{verbatim}

Response:
\begin{verbatim}
{
    "message": <status_message>
} 
\end{verbatim}

% todo: restoreQuery()

\subsubsection{Query Step 2: Data Selection}

In the PIC-SURE paradigm, this step is about preparing the \emph{select} statement of the query: the output for step 3 is the added \emph{select} causes to the query.

\subparagraph{\emph{select} Models}
A \verb|PICSURESelectAttribute| model is created to represent the \emph{select} clauses of PIC-SURE:
it holds the paths/datatype, and then offer methods to set if we want the counts or the aggregate, etc.

\paragraph{Generating \emph{select} Attribute}
The method \verb|ConstraintService.generateProjectionConstraint()| is originally what is used by other parts of the code to generate the projection constraints in a format that fits the API call.
We rename it to \verb|generateSelectAttribute()| to fit the PIC-SURE jargon, and modify the method accordingly.
It returns a \verb|PICSURESelectAttribute| as described in the previous paragraph?).


the modele pic sure select attr, has all the things selected from step 2, and after we set if we want count or what not 



%Component orchestrating the step 2 is originally the \verb|GbProjectionComponent|
QueryService.updateCounts_2
ConstraintService.generateSelectionConstraint() : generate constraint with inclusion / exclusion: rename to generateConstraint
% todo: criteria file import



\subsubsection{Query Step 3:  Data Export}

todo: data types for export? what is it exactly ? -> data format api call 


tabularresults
check how data table couljkd be done (process? join?)

%Component orchestrating the step 3 is originally the \verb|GbExportComponent|


GET /resultService/available: returns the list of available results (of the user)
returns the result id and the status
<<< GB does first the call to get job list

GET /resultService/resultStatus/ID
if problem: message field (always try to get it, but can be null)
if OK, fields are: result id, status, startime, endtime, data type

GET /resultService/availableFormats/ID
returns list of formats for a specific result

GET /resultService/result/ID/format?download=yes
download can be put to no (content type will change, set yes for download from browser with clickable link)


some formats may not be available, but could be implemented: do match and check with ward/ bo which to support

--> match of the columns? possible to cancel in irct?NO, status returned ? YES

GB original: 
/file\_formats
/data\_formats


\subsection{IRCT Resources Implementation}
some to be modified, some to be implemented

- add support for shrine/medco with additional JAR packaged and put in wildfly
- add systems (by type) with pq-psql functions

resources modficiatons: add the agreggates (i2b2 just count, avg/min/max seems to be complicated? check it out)
i2b2: no select (currently), because it is mainly about the constraints: where clauses

importatn to specify what is done with minimal set of things supported

--> include datatype in query: saves a req to tree node
\paragraph{i2b2}

\paragraph{SHRINE}

\paragraph{tranSMART 16.2}

\paragraph{tranSMART 17.1}
last step is transmart 17.1 as connector, because it is if time allows


%\subsection{Current Status of Softwares}
%\subsection{Glowing Bear}
%desc of how it works
resource info in db -> example on how to do it with i2b2
through the api resourceService/resources get us all the resources and their capabilities(by type, but we want query for now i guess?)
define:
- predicates
- relationships
- the entities also define relationships, what is it exactly?
-> for GB we would need a minimum set of capabilities??

-> study should become just a parameter of the tree entity, but we shouldnt care about it
-> how / at which stage implement that in Gb?

edu.harvard.hms.dbmi.bd2k.irct.cl.rest;
classes implementing the REST API, using javax.ws.rs / JAX-RS: Java API for RESTful Web Services

---

-- irct --
version from github to select: which? there seem to be a new query param (commit on feb 2) only\_count, which may prove valuable for GB << important question to solve, check out a specific commit? last release is not recent enough as of now 

--> in the queryesrvice, when running query, there is a parameter "only\_count" that exists (in the later version)
it is added to the metadata of the query, to be processed by the RI (i guess support not mandatory) 
-> to have less processing, use it from GB!

select aggregate (count), they exist, check out: https://github.com/hms-dbmi/IRCT/blob/ae31f79ef2015bc6ae349e30e2a6652a22141b07/IRCT-RI/src/main/resources/SciDB.sql

aggregate is a type of select, and count is a type of aggregate, totally OK to do that 



--Resources--
does a lib in java to talk to transmart rest v2 exists? would be nice

todo: i2b2 parsing of and/or check notes

todo: transmart cache the ontology because depth can be specified

implekmentaion of transmart: study as first layer? no just the tree presentede by transmart

define interface for java / sql skeleton for GlowingBearFeatureName (counts, etc.)


RI transmart: thourhg the path, if within study: study is queried, if cross-study: no study queried, basically do as the constraint is specifying
+ to find the study associated to a concept, simply go up, or keep in cache a list of studies
Moreover the tree concept of tranSMART has at its root the studies
expose in tree as DIMENSION or as STUDY? --> dimension (like this lcinical trial also supported)

todo transmart: clauses nest / unnest
todo transmart: study data type
todo transmart: studies list, select operation

% todo: in irct modifs: add query name to the saved queries / add getQueries that returns the whole queries / add getQuery/id with PUT, DELETE, methods


% todo: config of irct: in appendix? 

irct tree impl.: transmart include study
i2b2 include modifiers


from transmart api: OK to use concept path
 "type": "concept",
                "path": "\\Public Studies\\EHR\\Diagnosis\\",


---
i2b2 parse and or:
Y OR Z OR (A AND B)
(Y OR Z OR A) AND (Y OR Z OR B)

--> 1 lvl only nesting
todo: how to enforce genericly this structure in the UI? what is possible to do in UI?

---
enum / study / modifier :TREE