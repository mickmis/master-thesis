\newpage
% todo: add about transmart docker (in appendix maybe?)
% todo: explanation about data table
% todo: explanations+ modifs about transmart 16.2
% todo: integration in common interface
\section{Implementation of the Interoperability Layer}
Here is described component-per-component the implementation of the solution presented in the last section. 
Each of the sections first explains what is the current status of the relevant components, and then what needs to be modified or implemented.
First we describe the deployment of all the relevant components that need to be used in the system (refer to figure~\ref{fig:sysdiagramobj1}).
Then we are going over all the modifications made to Glowing Bear, regarding the change of the API it uses.
Finally we go over the modifications of the core of IRCT, and the implementation of all the resources it uses.


\subsection{Deploying Original Components}
% todo: transmart 16.2?
% todo: versions to be used

First of all we are compiling, setting up and deploying all the building blocks of the solution: Glowing Bear, IRCT, tranSMART 17.1, i2b2, SHRINE.
The exhaustive information about the deployments using Docker can be found in appendix~\ref{sec:docker-images}.
These components require some servers that are deployed with Docker: WildFly (application server), PostgreSQL (database server), lighttpd (web server)

% GB
We are deploying Glowing Bear from the sources using the Angular command \verb|ng serve|, which is practical for development.
For production deployment it is deployed through the web server.
The version used is branched off the development branch \verb|table|, which is the most up to date branch at the time of the writing, and is regularly rebased on this same branch.
It is configured to use the locally deployed instance of back end components.

% todo: put in annex for docker deployment, also db
Keycloak is deployed as an OpenID Connect server using its official Docker image that sets up a working instance.
It uses the same PostgreSQL database that other deployments are using.
The version used is 3.4.3.
It is configured with a local user data source managed by itself, and one client for each back end that will use this instance.

% IRCT
IRCT is compiled from sources and deployed using Docker.
The version used is a fork of the latest version on the master branch of the repository. 
It is regularly rebased to keep track of the changes.
It is configured to use one local instance of each type of supported back end systems: i2b2, tranSMART 17.1, SHRINE.
The configuration is done through the local PostgreSQL database.

% i2b2
I2b2 is compiled from sources and deployed using Docker.
The version used is TBD.
The Spring configuration files and test data comes from the demo dataset that is provided with i2b2, and allows to have a demonstration running instance, done through the local PostgreSQL database.

% tranSMART 17.1
tranSMART 17.1 is compiled from sources and deployed using Docker.
The version used is TBD.
The configuration is done through a grails configuration file, and some test data is loaded in the local PostgreSQL database used.

% SHRINE
First step of the SHRINE deployment it to modify the existing i2b2 deployment.
Its demo data (loaded in PostgreSQL) is duplicated three times, to replicate (with the appropriate configuration) three different instances of i2b2 (but served through the same web service).
Then three instances of the SHRINE web services are deployed: this corresponds to a setting where the SHRINE network has three nodes.
It is compiled from sources and deployed using Docker. 
The version used is TBD.
It is configured through simple configuration files.
The SHRINE instances use a deployment of the MySQL database server for their data.
The SHRINE webclient is served by the web server.

\subsection{Glowing Bear Modifications}
% todo: step 4 w/ data tables ?
% todo: this overview might belong in design rather than here? evaluate

% GB modif overview
Glowing Bear is pretty heavily modified, the idea being to make it a native PIC-SURE API client by replacing completely the tranSMART client API implementation. 
In general the API requests are modified through the calls in \verb|ResourceService|, and the response through the models in \verb|src/app/models/|.
On a very high-level, the important steps of the queries as mapped as such:
\begin{itemize}
    \item Step 1 (patients subselection with constraints): defining PIC-SURE \emph{where} clauses, making count queries queries with the \emph{select} clause
    \item Step 2 (selection of data to export): defining \emph{select} clauses for data export, making count queries as well
    \item Step 3 (data export): executing the export query previously built
\end{itemize}

% todo: save query is after step 2

% overview of steps taken in GB workflow: steps for modifications
The organization of this section follows Glowing Bear typical workflow:
\begin{itemize}
    \item Client initialization: loading and login
    \item Explore the tree of concepts
    \item Step 1: Construct a query or re-use a saved query
    \item Step 2: Select the data to export
    \item Save a constructed query
    \item Step 3: Export the data
\end{itemize}

% why no transmart rest v2
Compatibility with tranSMART versions 17.1+ is later restored through the implementation of an IRCT resource interface (see~\ref{sec:irct-res-transmart-17.1}).
The reasoning behind this choice is that maintaining compatibility of two different but similar APIs in Glowing Bear would be possible, but complicated, which translates into additional efforts spent on the implementation, and later on the maintenance of the code: this would be sub-optimal as these efforts are better spent elsewhere.
The potential downside of this choice is a time delay for the requests as we are introducing an additional middle-component, these are formally measured in a later chapter.%chapter~\ref{chap:perfeval}. % todo: ref


\subsubsection{Authentication \& Authorization}

IRCT uses OpenID Connect to authenticate and authorize the users of its client applications, while Glowing Bear originally implements the client side of the OAuth2 protocol for authorization with tranSMART. 
The modification to Glowing Bear is the migration from the authorization protocol OAuth2~\cite{oauth2} to the authentication and authorization protocol OpenID Connect~\cite{openidconnect}.
Since OpenID Connect is a layer on top of OAuth2, the modifications to migrate the client code from OAuth2 to OpenID Connect are not significant, and are made by making use of the \emph{angular-auth-oidc-client} library~\cite{angular-auth-oidc-client}.
The flow remains the same: Glowing Bear obtains a JSON Web Token (JWT) from the OpenID Connect provider, which will then be embedded in the header of the HTTP requests to IRCT.
These modifications are made by replacing the \verb|EndPointService| by \verb|AuthenticationService|, which implements the logic of the authentication.
Additionally a small component \verb|AutoLoginComponent| redirects to the login page of the OIDC server if the authentication token is not valid or not present.

\subsubsection{IRCT Resources Support}
% todo: summary of GB supports for predicates / operations

\paragraph{Resources List}
After the user has successfully logged in, Glowing Bear requests the list of IRCT resources available and their definitions.
Then the user chooses from the list which resource to use for the current session: it is possible to use only one resource at a time. % todo: behavior to be confirmed
The selected resource and its definition are kept in the new \verb|IRCTResourceService| service that keeps and processes all the information about the resource returned by PIC-SURE.
Among other things, this service is used to make the connection between tree data types and possible predicates that can be applied on them.
The method \verb|ResourceService.getResources()| is added to retrieve the PIC-SURE resources with the following API call:

\newpage
\begin{verbatim}
GET /resourceService/resources
\end{verbatim}

Response:
\begin{verbatim}
[
  {
    "id": 1,
    "name": "resource name (e.g. i2b2-local)",
    "implementation": "resource type (which implementation is used, e.g. i2b2XML)",
    "relationships": [ supported relationships between tree nodes, e.g. CHILD ],
    "logicaloperators": [ "AND", "OR", "NOT" ],
    "predicates": [ supported predicates and their properties, e.g.: {
        "predicateName": "predicate name (e.g. CONTAINS)",
        "displayName": "displayed name (e.g. Contains)",
        "description": "description",
        "default": true if this predicate should be selected by default,
        "fields": [ {
            "name": "field name (e.g. By Encounter)",
            "path": "field code (to be used in query, e.g. ENCOUNTER)",
            "description": "By Encounter",
            "required": true,
            "dataTypes": [ data type of the value field(s) of this predicate ],
            "permittedValues": [ permitted values, if it categorical ]
          } ],
        "dataTypes": [ data types to which this predicate applies (e.g. STRING) ],
        "paths": [ paths to which this predicate applies (empty for all) ]
      } ],
    "selectOperations": [ supported operations for select (e.g. AGGREGATE) ],
    "selectFields": [ supported fields for the operations, e.g. COUNT for AGGREGATE ],
    "joins": [ ],
    "sorts": [ ],
    "processes": [ ],
    "visualization": [ ],
    "dataTypes": [ data types and their properties, e.g.: {
        "name": "name of the data type",
        "pattern": "regex to validate the value",
        "description": "description of the data type"
    } ]
  },
  ...
]
\end{verbatim}


\paragraph{Specific Features}
\label{sec:specific-features}

\verb|IRCTResourceService| provides methods that determine if some specific features are supported, which allows some features of Glowing Bear to be enabled or disabled:
\begin{itemize}
    \item \verb|supportsCounts()|: returns \emph{true} if queries of type \verb|SELECT AGGREGATE COUNT| are supported, which enables the live counts display in the UI
    \item \verb|supportedCountsDimensions()|: if \verb|supportsCounts()| is \emph{true}, returns the countable dimensions (most commonly \emph{Patients} and \emph{Observations}).
    \item \verb|supportsNestedClauses()|: returns \emph{true} if the \emph{where} clause predicates \verb|CLAUSE_NEST| and \\
    \verb|CLAUSE_UNNEST| allowing nesting are supported
    \item \verb|supportsMinMax()| returns \emph{true} if queries of type \verb|SELECT AGGREGATE MIN/MAX| are supported
    \item \verb|supportsStudies()| / \verb|supportsClinicalTrials()|: returns \emph{true} if queries based on those specific dimensions are supported 
    \item \verb|supportsPedigrees()|: returns \emph{true} if constraints based on pedigrees are supported
    \item \verb|supportsGetTreeWithDepth()|: returns \emph{true} if the tree can be retrieved with a specified depth (if the resource declare a tree relationship as \verb|CHILDREN-DEPTH-X|)
    \item \verb|supportsDataExport()|: returns \emph{true} if data export queries are supported
\end{itemize}


\subparagraph{tranSMART Studies}

% general + into tree
The tranSMART REST API v2 has several calls that are specific to studies, while the PIC-SURE API does not have a direct equivalent to this.
However since the studies are completely embedded within the concept tree, i.e. a concept belongs to one study or it is cross-study, they can simply be abstracted into the tree of entities exposed by PIC-SURE: we create a \verb|STUDY| data type for the tree entities (see~\ref{sec:gb-tree}).
\verb|TreeNodeService.isTreeNodeAStudy()| implements the recognition of the \verb|STUDY| data type.
%todo: in model

% list of studies
If the resource supports it, \verb|ConstraintService.loadStudies()| loads the list of studies by calling \verb|ResourceService.getStudies()|, which is modified to use the following PIC-SURE call:
\begin{verbatim}
POST /queryService/runQuery
{
  "select": [ {
    "operation": "AGGREGATE",
    "fields": {
        "FUNCTION": "values",
        "DIMENSION": "study"
    }
  } ],
  "alias": "get_studies"
}
\end{verbatim}

%\subparagraph{Pedigree / Relation Type}
% todo: GB asks transmart for the relation types, but for now we are putting them into permitted values of irct definition, change that ?


\subsubsection{Concepts Tree}
\label{sec:gb-tree}
% todo: for i2b2 (when no greedy loading possible): node per node loading
% todo: this means also no constraints list (dropdown) available / also tree search
% transmart keeps the depth loading

% todo: say mechanism
% in GB: stay as currently, loading everything in background 
% in IRCT: we cache the tree (in tree service to have it once?)
% or load it during the setup?
% problem: tree may be user specific!!!
%transmart: cache is study specific, i2b2: is project specific
% from GB side: either load everything, or ndoe per node???
%from GB: ALL-CHILDREN (and not depthxx) todo
% irct: support child and all-children
%for i2b2 that will be sloow to get all nodes on the server side
% also need for step 2 the tree

% modification of request (api call)
The API call in \verb|ResourceService.getTreeNodes()| is modified according to the following:
\begin{itemize}
     \item \verb|depth| parameter is replaced by \verb|relationship|: if the resource supports the retrieval of nodes with a depth more than one, then it will be achieved through a specific entities relationship API call \verb|CHILDREN-DEPTH-X|; 
    \item parameters \verb|hasCounts| and \verb|hasTags| are removed (these fields are always returned, although can be empty according to the resource implementation)
\end{itemize}

The API request becomes:
\begin{verbatim}
GET /resourceService/path/<resource>/<path>/?relationship=<relationship>
\end{verbatim}

Response:
\begin{verbatim}
[
  {
    "pui": "/<resource>/<path>/",
    "name": "Concept internal name",
    "displayName": "Concept name",
    "description": "Concept description",
    "ontology": "Ontology Code",
    "ontologyId": "Concept ID in the ontology",
    "relationships": [ supported relationships ],
    "counts": {},
    "dataType": {
      "name": "Data type name",
      "pattern": "Validation Regex",
      "description": "Data type description"
    },
    "attributes": {
      "visualattributes": "Visual attributes",
      "customAttributeName": "customAttributeValue"
    }
  },
  ...
]
\end{verbatim}

The \verb|dataType| field allows to know which constraints defined in the resource can be applied, thus what options can be presented to the user when a query is constructed with the help of the \verb|IRCTResourceService|.
The \verb|visualattributes| field allows to modify the appearance of the concept in the UI, for example if it's a folder containing concept, or a leaf node. 
Its presence is optional so if it is absent, the appearance will stay as it is by default, this behavior is defined in the \verb|GbTreeNodesComponent| component.
The other fields can easily be mapped to original fields in Glowing Bear.

% modification relating no depth call possible
In \verb|TreeNodeService.loadTreeNodes()|, \verb|loadTreeNext()| is called to iteratively load the nodes.
\verb|loadTreeNext()| needs to be modified to account for \verb|ResourceService.getTreeNodes()| possibly not being able to load with a depth greater than one, meaning it must be called for every node of the tree.
This is determined using \verb|IRCTResourceService.supportsGetTreeWithDepth()|.
The difference between a node and a leaf is made with the use of the \verb|relationships| field: if the node supports the relationship \verb|CHILD|, it is a node for which children can be requested.

% about format of node JSON
\verb|TreeNodeService.processTreeNodes()| and \verb|processTreeNode()| process the received JSON to load it into the internal \verb|treeNodes| array containing the tree in memory, they should be adapted to fit the PIC-SURE JSON format.
This allows all the other parts of Glowing Bear that use the node to access the information needed.


\subsubsection{Query Step 1: Constraints}
% todo : concept_enum only, values are requested

% GB overall workflow for queries
The original Glowing Bear workflow for creating the constraints is the following: \\
\verb|GbConstraintComponent.onDrop()| processes the node being dropped in the query construction panel in the UI by calling \verb|ConstraintService.generateConstraintFromSelectedNode()| to generate the constraint based on the dropped node. \\
It uses \verb|ConstraintService.generateConstraintFromConstraintObject()| to construct the individual constraint objects.

The constraints generated in the step 1 of the query corresponds to the \emph{where} clauses of the PIC-SURE query: they define the criterion the resulting data must satisfy.
The components based on \verb|modules/gb-data-selection-module/constraint-component/GbConstraintComponent| and the models based on \verb|models/constraint-models/Constraint| correspond to the different PIC-SURE predicates that Glowing Bear supports.
Overall the Glowing Bear workflow stays the same at a high-level, but its implementation at the low-level undergoes significant changes to bring compatibility with PIC-SURE.

% logic
\verb|ConstraintService| is initialized with \verb|IRCTResourceService|: this allows the service to link the data types of the tree nodes with the constraints they support. \\
\verb|ConstraintService.generateConstraintFromConstraintObject()| is the core method of the constraint generation, and is thus completely re-implemented and is renamed \\
\verb|generateConstraintFromDataType()|.
It is modified to take as input the data types coming from the PIC-SURE tree, and is using the \verb|IRCTResourceService| to get the constraints corresponding to the data types, returning a \verb|Constraint| object.
\verb|generateConstraintFromSelectedNode()| is modified to have two cases only: it is a queryable node, i.e. it has a data type, or not. If it has a data type it uses \verb|generateConstraintFromDataType()| to get the constraint, if not it calls itself recursively with the children nodes.

% data types supported
\verb|generateConstraintFromDataType()| supports the following PIC-SURE data types:
\begin{itemize}
\item Primitive
    \begin{itemize}
        \item Numeric types: \verb|INTEGER|, \verb|LONG|, \verb|FLOAT|, \verb|DOUBLE|
        \item Date types: \verb|DATE|, \verb|DATETIME|
        \item String type: \verb|STRING|
    \end{itemize}

\item Custom
    \begin{itemize}
        \item Enumerated type: \verb|ENUM_FIELD| and \verb|ENUM_VALUE| (enumerated value exposed through the tree and not as a value)
        \item Ontology concept type: \verb|CONCEPT| (simple concept without value)
        \item Study: \verb|STUDY| (restrict to a specific study)
        \item Pedigree: \verb|PEDIGREE| (constraint based on relationship, e.g. parents of another selection of patients)
    \end{itemize}
\end{itemize}


\paragraph{\emph{where} Models}

% constraint models
The way the constraints are represented internally need modification, as the nature of the tranSMART constraints and the PIC-SURE \emph{where} clauses are slightly different: they are more generic, but more importantly the supported predicate for each data type are known only at the runtime.
Constraints models in \verb|src/app/models/constraint-models/| are now not based on the type of the constraint, but on the predicate used for the constraint.
The interface \verb|Constraint| remains, but the members \verb|toQueryObjectWithSubselection()|, \verb|toQueryObjectWithoutSubselection()| and \verb|parent| are removed.
The equivalent of the subselection in PIC-SURE would be the dimension, but 
\begin{enumerate*}[label=(\arabic*)]
  \item it is defined by the resources themselves,
  \item it is integrated into the \emph{select} clauses, which is handled in the second step as it is not considered a constraint;
\end{enumerate*}
justifying the removal of those members.
They also now contain data about themselves specified by the IRCT resource: predicates name, description, paths and data types it applies to, its fields (name, code, description, required flag, data type and permitted values).

% constraint models JSON generation example
\verb|toQueryObject()| takes care of generating the JSON of the constraint: all the implementing methods now needs to generate the PIC-SURE \emph{where} clauses.
Example of two different \emph{where} clauses that would be produced by the corresponding \verb|toQueryObject()| methods:
\begin{verbatim}
{
    "field": {
        "pui": "/transmart/study1/Gender/Male/",
        "dataType": "ENUM_VALUE"
    },
    "predicate": "CONTAINS"
}

{
    "field": {
        "pui": "/transmart/study1/Age/",
        "dataType": "INTEGER"
    },
    "predicate": "CONSTRAIN_VALUE_NUMERIC",
    "fields": {
        "OPERATOR": "==",
        "CONSTRAINT": "5"
    }
}
\end{verbatim}


% ----------------------------------------------------------------------------------
\paragraph{Instantiating Constraints}
\label{sec:gb-instanciating-constraints}

Because the predicates and the fields they have are known only at runtime, we create a service \verb|ConstraintFactoryService| that handles the instantiating of properly initialized \verb|Constraint| objects.
A method \verb|createConstraint()| taking as parameters the data type and and predicate returns the \verb|Constraint| object.
While some pre-defined predicates are supported by Glowing Bear (see list below), or even required for some features, not all can be supported. 
For this reason a generic \verb|Constraint| is created, that allows Glowing Bear to handle unknown constraints that resources might declare.

% list of types of constraints, based on the predicate they support
The different constraint models with their associated predicate, that together support all the PIC-SURE data types listed before, are listed below:
\begin{itemize}
    \item \verb|ConceptConstraint|: predicate \verb|CONSTRAINT_CONCEPT|, valid for \verb|ENUM_FIELD|, \verb|ENUM_VALUE|, \verb|CONCEPT| and all primitive types 
    \item \verb|FieldConstraint|: predicate \verb|CONSTRAINT_FIELD|, valid for arbitrary values of fields in dimensions (e.g. data types \verb|STUDY|, \verb|TRIAL_VISIT|), and used to create constraint based on observation date
    \item \verb|PedigreeConstraint|: predicate \verb|CONSTRAINT_PEDIGREE|, valid for pedigree data types
    \item \verb|PatientSetConstraint|: predicate \verb|CONSTRAINT_PATIENT_SET|, valid for patient set (imported through the UI)
\end{itemize}

Note that \verb|StudyConstraint| and \verb|TrialVisitConstraint| are merged within the new \verb|FieldConstraint|, similarly \verb|GbStudyConstraintComponent| into the new \verb|GbFieldConstraintComponent|.
The trial-visit logic form \verb|GbConceptConstraintComponent| is moved to \verb|GbFieldConstraintComponent|.
Note also that not all constraints are supported by all resources, the support is known by using \\
\verb|IRCTResourceService.supports*()| methods. 

\subparagraph{Concept Constraint}
This is a simple constraint based on the presence of a concept and possibly its associated value.
A change from the original behavior is the way the values of the enumerated field are recuperated: this is done through the tree by looking at the data types, enumerated fields have a \verb|ENUM_FIELD| types, and its possible values are children of the node with the type \verb|ENUM_VALUE|.
Another change concerns the aggregates values, see~\ref{sec:gb-step1-aggregates} for more information.
The supported operators are \verb|==|, \verb|<=|, \verb|>=|, \verb|<|, \verb|>|, \verb|LIKE[exact]|, \verb|LIKE[begin]|, \verb|LIKE[end]| and \verb|LIKE[contains]|.

Example of concept \emph{where} clauses:
\begin{verbatim}
{
    "where": [ {
            "field": {
                "pui": "/transmart/study1/Age/",
                "dataType": "INTEGER"
            },
            "predicate": "CONSTRAIN_CONCEPT", 
            "fields": { "OPERATOR": ">=", "VALUE":"20" } 
        }, {
            "field": {
                "pui": "/transmart/study1/Age/",
                "dataType": "INTEGER"
            },
            "predicate": "CONSTRAIN_CONCEPT", 
            "fields": { "OPERATOR": "<=", "VALUE":"25" },
            "logicalOperator": "AND"
        }, {
            "field": {
                "pui": "/transmart/study1/Gender/Male/",
                "dataType": "ENUM_VALUE"
            },
            "predicate": "CONSTRAIN_CONCEPT"
        } ]
}
\end{verbatim}

\subparagraph{Field Constraint}
This new constraint allows to restrict according to the value of field in some dimension of the data.
It allows to query for a study, clinical-trial visit, observation date, and others.
It is implemented in the constraint model \verb|FieldConstraint| and the component \verb|GbFieldConstraintComponent|.

Example of field \emph{where} clauses generated:
\begin{verbatim}
{
    "where": [ {
            "predicate": "CONSTRAIN_FIELD", 
            "fields": { 
                "DIMENSION": "study", "FIELD":"study_id", 
                "OPERATOR": "==", "VALUE": "ORACLE_1000_PATIENT" 
            } 
        }, {
            "predicate": "CONSTRAIN_FIELD", 
            "fields": { 
                "DIMENSION": "trial_visit", "FIELD":"rel_time_num", 
                "OPERATOR": "==", "VALUE": "2" 
            },
            "logicalOperator": "AND"
        }, {
            "predicate": "CONSTRAIN_FIELD", 
            "fields": { 
                "DIMENSION": "observation", "FIELD":"start_date", 
                "OPERATOR": "<=", "VALUE": "2010-02-22" 
            },
            "logicalOperator": "AND"
        } ]
}
\end{verbatim}

\subparagraph{Pedigree Constraint}
Constraints based on pedigree are special in that they are based on other constraints (e.g. getting the parents of patients aged more than 50).
We add for them a predicate \verb|CONSTRAINT_PEDIGREE|, which has three fields:
\begin{itemize}
    \item \verb|TYPE|, required: list of permitted values which are either the type of pedigree (parents of, siblings of, etc.) or the indicator of the end of the pedigree constraints \verb|CONSTRAINT_END| (see example after)
    \item \verb|BIOLOGICAL|, optional, default to \verb|BOTH|: either \verb|YES|, \verb|NO| or \verb|BOTH|
    \item \verb|SHARE_HOUSEHOLD|, optional, default to \verb|BOTH|: either \verb|YES|, \verb|NO| or \verb|BOTH|
\end{itemize}

Example of a pedigree \emph{where} clause:
\begin{verbatim}
{
    "where": [
        {"predicate": "CONSTRAINT_PEDIGREE", "fields": { 
          "TYPE": "PARENTS_OF", "BIOLOGICAL":"YES" 
        } },
        <classic constraints>..., 
        {"predicate": "CONSTRAINT_PEDIGREE", "fields": { "TYPE": "CONSTRAINT_END" } }
    ]
}
\end{verbatim}

Similarly to other constraints, the logic construction is implemented into \\
\verb|GbPedigreeConstraintComponent| and the model representing such a constraint is \verb|PedigreeConstraint|.
It is only enabled if \verb|IRCTResourceService.supportsPedigree()| determines so.

\subparagraph{Patient Set Constraint}
Through the UI, the user of Glowing Bear can import a patient set and use it as a constraint, this handled by \verb|GbPatientSetConstraintComponent| and stored into a model \verb|PatientSetConstraint|, which are modified to adapt to the PIC-SURE format.
It supports either an array of patient identifiers, or the identifier of a patient set.

Example of patient set \emph{where} clauses:
\begin{verbatim}
{
    "where": [
        {"predicate": "CONSTRAINT_PATIENT_SET", "fields": { "PATIENT_SET_ID": "52" } },
        {"predicate": "CONSTRAINT_PATIENT_SET", "fields": { 
          "PATIENT_IDS": "[2, 32, 96]" 
        } }
    ]
}
\end{verbatim}


% ----------------------------------------------------------------------------------
\paragraph{Logical Operators}
\label{sec:gb-logical-operators}

Glowing Bear supports the definition of nested inclusion criteria, with the criteria belonging to the same group being linked by a logical operator \emph{AND} or \emph{OR}.
PIC-SURE allows queries with several \emph{where} clauses and lets each resource declares the logical operators it supports to link them. 
However the link between the clauses is flat, a clause defines its relationship with the previous clause: nested queries are not possible natively with PIC-SURE, but a workaround is presented below.

For resources that support nested queries, they declare the support for the \verb|NESTING| predicate, having a field called \verb|TYPE| with the permitted values \verb|START| and \verb|END|.
By using these it is possible for resources to support nested queries, see the following example for the methodology:
Consider the following nested query constructed in Glowing Bear:
\begin{verbatim}
    ((H AND B) OR (D AND S)) AND X AND (H OR F)
\end{verbatim}
It would have the PIC-SURE query with the following \emph{where} clauses:
\begin{verbatim}
{
    "where": [
        {"predicate": "NESTING", "fields": { "type": "START" } },
        {"predicate": "NESTING", "fields": { "type": "START" } },
        { H },
        { B, "logicalOperator": "AND" },
        {"predicate": "NESTING", "fields": { "type": "END" } },
        {"predicate": "NESTING", "fields": { "type": "START" }, "logicalOperator": "OR" },
        { D },
        { S, "logicalOperator": "AND" },
        {"predicate": "NESTING", "fields": { "type": "END" } },
        {"predicate": "NESTING", "fields": { "type": "END" } },
        { X, "logicalOperator": "AND" },
        {"predicate": "NESTING", "fields": { "type": "START" }, "logicalOperator": "AND" },
        { H },
        { F, "logicalOperator": "AND" },
        {"predicate": "NESTING", "fields": { "type": "END" } }
    ]
}
\end{verbatim}

\verb|CombinationConstraint|, \verb|GbConstraintComponent| and \\
\verb|ConstraintService.generateConstraintFromConstraintObject()| are modified to...
\begin{itemize}
    \item support this construction by storing an array of \verb|Constraint|, appropriately setting their \\
    \verb|logicalOperator| fields and adding the nesting \emph{where} clauses;
    \item allowing or not the nesting of queries according to the resource capabilities.
\end{itemize}
Query nesting status is reflected in the UI by removing the \verb|add criterion| boxes for the attributes with a level equal to or lower than 1 if it is not supported.

% generation of the whole thing from outside
The method \verb|ConstraintService.generateSelectionConstraint()| is what is used by other parts of the code to generate the constraints defined in the first step in a format that fits the API call.
We rename it to \verb|generateWhereAttribute()| to fit the PIC-SURE jargon, and modify the method accordingly.
It returns a \verb|CombinationConstraint| as described in the previous paragraph.

\subparagraph{Negation}
The logical operator \verb|NOT| is at the same level as the \verb|AND| and \verb|OR|.
For this reason the resources declare all the following operators:
\begin{enumerate*}[label=(\arabic*)]
  \item \verb|AND|,
  \item \verb|OR|,
  \item \verb|NOT|,
  \item \verb|AND NOT|,
  \item \verb|OR NOT|.
\end{enumerate*}
Which allows all kinds of queries.


% ----------------------------------------------------------------------------------
\paragraph{User Interface}
When adding criterion at the step 1, the UI has to know what data type is the entity in order to know what input from the user is expected.
This behavior is implemented in the \verb|GbConstraintComponent| and its extending components.
These components are modified or removed to fit all the modifications previously described, to arrive at a state where there is one component for each of the supported predicates described section~\ref{sec:gb-instanciating-constraints}.
Additionally they use the information provided by \verb|IRCTResourceService| to:
\begin{itemize}
    \item offer to the user a list of the supported predicates to choose from,
    \item know what fields the predicate needs,
    \item enforce format of the fields input values with the regex or offer a dropdown list of permitted values, 
    \item enforce required or optional fields,
    \item display aggregates about the concept (such as the min, max, etc.).
\end{itemize}

The parent \verb|GbConstraintComponent| is modified to hold the data type of the dropped node, and allow for the switch between predicates (if multiple predicates are supported by the data type).
Then the extending components, one for each predicate (with the addition of the one for unknown predicates), are used according to the chosen predicate.

% todo: mention modifiers (but expose through tree)
% todo: create model SelectClause (for count example: fields pui, dataType, Function, dimension)
% todo: rename constraint to whereclause
% todo: these last 2 things, check with the common models they are doing currently

\subsubsection{Query Step 1: Aggregates}
\label{sec:gb-step1-aggregates}
%todo: model SelectClause

Glowing Bear does three kinds of aggregate queries: \emph{counts}, \emph{min} / \emph{max} and \emph{values}. 
\emph{Counts} queries are made when the user presses \emph{Update Counts} after modifying constraints (in step 1) or after modifying selected attributes for export (in step 2).
\emph{Min} / \emph{max} and \emph{values} queries are made when constructing constraints from the UI, to help the user the user by displaying some metadata.
The API calls and calling methods made are modified to use PIC-SURE as described below.
As support for aggregate \emph{select} calls is not mandatory for the resources, if the resource does not support it the related features are disabled (more info in section~\ref{sec:specific-features}).

\verb|ResourceService.getCounts()|, \verb|getStudies()| are merged into \verb|getAggregate()|.
This means that all the code calling those methods need to be adapted to fit the inputs and outputs of the new \verb|getAggregate()|.
This notably includes \verb|GbConceptConstraintComponent.initializeConstraints()|, \verb|QueryService.updateInclusionCounts()|, \verb|updateExclusionCounts()|, \verb|updateCounts_2()|, \\
\verb|updateConceptsAndStudiesForSubjectSet()|, \verb|ConstraintService.loadStudies()|.

\verb|getAggregate()| becomes more generic and accepts the following arguments:
\begin{itemize}
    \item \verb|aggregateType|: \emph{count}, \emph{min}, \emph{max}, \emph{values}
    \item \verb|dimension|: among the dimensions declared by the resource
    \item \verb|pui|: optionally a concept path if it is relevant for the query
    \item \verb|dataType|: data type of the \verb|pui|
\end{itemize}

% todo: mention the where clause in example
Example of PIC-SURE aggregate \emph{select} clauses:
\begin{verbatim}
POST /queryService/runQuery
{
  "select": [ {
    "field": { "pui": "/path/to/concept/", "dataType": "STRING" },
    "operation": "AGGREGATE",
    "fields": { "FUNCTION": "count",  "DIMENSION": "patient" }
  }, {
    "operation": "AGGREGATE",
    "fields": { "FUNCTION": "count", "DIMENSION": "observation" }
  }, {
    "field": { "pui": "/path/to/concept/", "dataType": "INTEGER" },
    "operation": "AGGREGATE",
    "fields": { "FUNCTION": "min" }
  }, {
    "field": { "pui": "/path/to/concept/", "dataType": "INTEGER" },
    "operation": "AGGREGATE",
    "fields": { "FUNCTION": "max" }
  }, {
    "operation": "AGGREGATE",
    "fields": { "FUNCTION": "values",  "DIMENSION": "study" }
  }  ],
  "where": [ <constraints> ],
  "alias": "<query_alias>"
}
\end{verbatim}


\subsubsection{Query Step 2: Data Selection}

% overview
In the PIC-SURE paradigm, this step is about preparing the \emph{select} statement of the query: the output for step 3 is the added \emph{select} causes to the query.
The changes in this part are minimal, as they are mainly about the modification of the resulting JSON containing the concepts the user wishes to get as a result, but they represent the same information.

% modifications
Originally this information is stored in a \verb|CombinationConstraint|, with individual constraints linked by a \verb|OR|.
Here for this purpose we are creating a new \verb|PICSURESelectAttribute| model containing the different \emph{select} clauses, which for each clause holds the path of the concept and the associated data type.
For the sake of consistency the method \verb|ConstraintService.generateProjectionConstraint()| is renamed to \verb|generateSelectAttribute()| and returns a \verb|PICSURESelectAttribute|.
Some additional minor modifications are made in the methods using this, such as \verb|QueryService.updateCounts_2()|, \verb|updateExports()| and \verb|GbExportComponent.runExportJob()|.

% example
Example of \emph{select} clauses generated during the second step:
\begin{verbatim}
"select": [ {
    "field": {
        "pui": "/<resource>/Public Studies/CATEGORICAL_VALUES/Demography/Age/",
        "dataType": "INTEGER"
    }
    }, {
    "field": {
        "pui": "/<resource>/Public Studies/CATEGORICAL_VALUES/Demography/Gender/Male/",
        "dataType": "ENUM_VALUE"
    }
    }, {
    "field": {
        "pui": "/<resource>/Public Studies/CATEGORICAL_VALUES/Demography/Gender/Female/",
        "dataType": "ENUM_VALUE"
    }
} ]
\end{verbatim}


\subsubsection{Saving a Query}
On top of modifying the \verb|Query| model to fit the PIC-SURE format, only the API calls to handle query saving need to be modified, the calling code remains the same.
The IRCT implementation to save and re-use saved queries is not complete: it is originally possible to only save queries, but not list or load them. 
See section~\ref{sec:irctsavedqueries} for the additional implementation in IRCT to add these features.

% saveQuery
In \verb|ResourceService.saveQuery()| the API request becomes:
\begin{verbatim}
POST /queryService/queries
{
    "queryName": <query_name>,
    "query": {
        <query_body>
    }
}    
\end{verbatim}

Response:
\begin{verbatim}
{
    "queryId": <query_id>
}    
\end{verbatim}

% getQueries
In  \verb|ResourceService.getQueries()| the API request becomes:
\begin{verbatim}
GET /queryService/queries
\end{verbatim}

Response:
\begin{verbatim}
{
    [
        "queryId": <query_id>,
        "queryName": <query_name>,
        "query": {
            <query_body>
        }
    ], [
        ...
    ],
    ...
}    
\end{verbatim}

% updateQuery
In  \verb|ResourceService.updateQuery()| the API request becomes:
\begin{verbatim}
PUT /queryService/queries/<query_id>
{
    "queryName": <query_name>,
    "query": {
        <query_body>
    }
}
\end{verbatim}

Response:
\begin{verbatim}
{
    "message": <status_message>
} 
\end{verbatim}

% deleteQuery
In  \verb|ResourceService.deleteQuery()| the API request becomes:
\begin{verbatim}
DELETE /queryService/queries/<query_id>
\end{verbatim}

Response:
\begin{verbatim}
{
    "message": <status_message>
} 
\end{verbatim}

% todo: restoreQuery()

\subsubsection{Query Step 3:  Data Export}

% intro / problem with pic-sure / overview of what is needed
Using data export with only the native \emph{select} and \emph{where} clauses of the PIC-SURE API reveals a bit limiting as IRCT originally supports only tabular results, i.e. a classic row-columns result.
Moreover there is no support for custom export parameters such as different types of data, only format of the result can be chosen between \emph{TABULAR} or \emph{JSON}.
On the other end, backends such as tranSMART supports elaborated parameters for getting results, which is a required features for our system.
For these reasons we are not using the native export mechanism of PIC-SURE in order to select the format of the data, but are defining a custom \emph{process} clause named \verb|EXPORT|, if the resource supports it.
Modifications in Glowing Bear to use PIC-SURE can be summarized in three points:
\begin{itemize}
    \item Get available data types and file formats for a specific query (export parameters);
    \item Submit export request;
    \item List exports and download them.
\end{itemize}

\paragraph{Available Export Parameters}

% API calls
\verb|ResourceService.getExportFileFormats()| requests to the back end the supported file formats for the exports.
\verb|getExportDataFormats()| requests to the back end the supported types of data to export, given the current query.
These two requests are merged into \verb|getExportFormats()| and the API calls are replaced by using a PIC-SURE query with specific \emph{select} clauses using the operation \verb|EXPORT|, example:
\begin{verbatim}
POST /queryService/runQuery
{
  "select": [ {
    "operation": "EXPORT",
    "fields": { "SUPPORTED_FORMATS": "[data, file]" }
  }, 
    <query_select_clauses>
  ],
  "where": [ <query_where_clauses> ],
  "alias": "<query_alias>"
} 
\end{verbatim}

% in service
\verb|QueryService.updateExports()| is modified to take into account the merging of these two methods and the result that is now represented differently.

\paragraph{Export Request}

% API calls
\verb|ResourceService.createExportJob()| and \verb|runExportJob()| that respectively create an run an export job are merged into \verb|runExportJob()|.
The call made is similar the previous, using \emph{select} clauses to specify the parameters of the export job:
\begin{verbatim}
POST /queryService/runQuery
{
  "select": [ {
    "operation": "EXPORT",
    "fields": { "data": "clinical", "file": "TSV" }
  }, {
    "operation": "EXPORT",
    "fields": { "data": "mrna", "file": "JSON" }
  }, 
    <query_select_clauses>
  ],
  "where": [ <query_where_clauses> ],
  "alias": "<query_alias>"
} 
\end{verbatim}

Note that the name of the fields match the possible values of the \verb|SUPPORTED_FORMATS| field.

% in component
\verb|GbExportComponent.runExportJob()| is modified to take into account the merging of these two methods and the result that is now represented differently.


\paragraph{Manage Exports}

% list
\verb|GbExportComponent.updateExportJobs()| takes care of refreshing the list of exports previously made by using \verb|ResourceService.getExportJobs()| to make the API request.
This API request is modified to:
\begin{verbatim}
GET /resultService/available
\end{verbatim}

% download
\verb|GbExportComponent.downloadExportJob()| take care handling the download of the file containing the exported results, and to do so it uses the API request made in \verb|ResourceService.downloadExportJob()|.
This call is modified to:
\begin{verbatim}
GET /resultService/result/<result_id>/ZIP?download=yes
\end{verbatim}

Note that before that a first call is made to get the available formats for the results, but in the case of multi-dimensional data it will always return \verb|ZIP|:
\begin{verbatim}
GET /resultService/availableFormats/<result_id>
\end{verbatim}

% todo: cancel / archive not covered
% todo: data table, using param in select EXPORT? process?


\subsection{IRCT-Related Implementation}

\subsubsection{IRCT Core Modifications}
% todo: version from github to select: which? there seem to be a new query param (commit on feb 2) only\_count, which may prove valuable for GB << important question to solve, check out a specific commit? last release is not recent enough as of now 
% todo: auth mechanism? TBD
% todo: no need for modif resources, we can access the user token from the resource (user is available as a field)

\paragraph{Saving Queries: Additional API Calls}
\label{sec:irctsavedqueries}

% rest api
The original IRCT does implement an API call to save a specific query: \verb|POST /queryService/savequery|.
However it does not provide a way to manage the existing queries.
As this is a required feature, we are adding it in the IRCT core.
We modify the class \verb|edu.harvard.hms.dbmi.bd2k.irct.cl.rest.QueryService| to add the following methods:
\begin{itemize}
    \item \verb|getQueries()|, implementing API call \verb|GET /queryService/queries|
    \item \verb|saveQuery()|, implementing API call \verb|POST /queryService/queries|
    \item \verb|deleteQuery()|, implementing API call \verb|DELETE /queryService/queries/<query_id>|
    \item \verb|updateQuery()|, implementing API call \verb|PUT /queryService/queries/<query_id>|
\end{itemize}

The mapping API calls to methods is done using JAX-RS~\cite{wiki:jaxrs}.

% controller
The class \verb|irct.controller.QueryController| that implements the logic of managing queries.
It needs to be modified so that the saved queries are associated to an user: the users should be able to manage only their own queries.

\paragraph{Data Export: Support of Additional Data Type}

% why + add type
The original IRCT supports results of following types: \verb|TABULAR|, \verb|JSON|, \verb|HTML|, \verb|IMAGE|.
Supports for types of results is represented through the enum \verb|edu.harvard.hms.dbmi.bd2k.irct.model.result.ResultDataType| and the the code that manipulates it.
The data exports in tranSMART and i2b2 gives multi-dimensional data that are not supported with this construction.
To support such results, we add an IRCT result type \verb|ZIP|.
The resources using this type declare it through their \verb|irct.model.resource.implementation.| \\
\verb|QueryResourceImplementationInterface.getQueryDataType()| method.

% things to modify as consequence for added type
In the newly created package \verb|irct.model.result.zip| we create a new class that implement the \verb|ZIP| result type, to do do it implements the classes
\verb|irct.model.result.Data| and \\
\verb|irct.model.result.Persistable|.
This is a pretty simple implementation that only stores the resulting files from resources on the file system.

\paragraph{Authentication Modifications}
TBD: modifications to support the authentication design
% todo


\subsubsection{IRCT Resources Implementation}

% overview
The IRCT resources needs to define two things:
\begin{itemize}
    \item declaration of its parameters and the kind of requests it supports, through SQL statements loaded in the database (we are using PostgreSQL);
    \item a Java class implementing some interfaces in a way that match the declaration of the resource capabilities.
\end{itemize}

Some of these things are already existent, but needs modifications, and some others need to be implemented from scratch.
The details of this is explained in the following paragraphs.

% java interfaces
The interfaces that are to be implemented by the resources are the following:
\begin{itemize}
    \item \verb|ResourceImplementationInterface|: generic resource, provides methods for setup and type of resource
    \item \verb|PathResourceImplementationInterface|: methods for traversing tree exposed by the resource
    \item \verb|QueryResourceImplementationInterface|: methods for running queries
    % \item \verb|ProcessResourceImplementationInterface|: methods for running processes
\end{itemize}

\paragraph{i2b2}
The i2b2 resource implementation exists in the original IRCT, along with a library that allows to communicate with i2b2.
However this implementation is not exactly adapted for our goal and needs modifications.

We make the i2b2 resource declare the following \emph{select} operations:
\begin{itemize}
    \item \verb|AGGREGATE|, fields:
    \begin{itemize}
        \item \verb|FUNCTION|, mandatory, possible values:
        \verb|COUNT|
        
        \item \verb|DIMENSION|, optional, possible values:
        \verb|observation|,
        \verb|patient|
    \end{itemize}
    
    \item \verb|EXPORT|, fields:
    \begin{itemize}
        \item \verb|SUPPORTED_FORMATS|: possible values: \verb|file| %, \verb|data|
        \item \verb|file|: value among the ones returns with \verb|SUPPORTED_FORMAT|
    \end{itemize}
    
    \item (without predicate): select data to export
\end{itemize}

And the following \emph{where} predicates:
\begin{itemize}
    \item \verb|CONSTRAIN_CONCEPT|, constraint based on a concept, field:
    \verb|OPERATOR|, optional, possible values:
    \begin{itemize}
        \item numeric values: \verb|==|, \verb|<=|, \verb|>=|, \verb|<|, \verb|>|
        \item string values: \verb|LIKE[exact]|, \verb|LIKE[begin]|, \verb|LIKE[end]|, \verb|LIKE[contains]|
    \end{itemize}
    
    \item \verb|CONSTRAINT_PATIENT_SET|, constraint based on a patient set, fields:
    \begin{itemize}
        \item \verb|PATIENT_SET_ID|: identifier of a patient set stored in the back end
        \item \verb|PATIENT_IDS|: array of patient identifiers
    \end{itemize}
\end{itemize}

% tree
Browsing the i2b2 tree of concepts if already implemented and does not need modification, except from the visual attributes parameters that is adapted to fit with a common standard.

% queries: select / where
The modifications are mainly targeted at fitting the exposed \emph{where} predicates and \emph{select} operations.
However there is a larger modification, which is to adapt the parsing of the \emph{AND} / \emph{OR} queries as described section~\ref{sec:gb-logical-operators} as they need to be re-organized in the resource implementation to fit the native i2b2 way of querying, which has a non flexible syntax:
\begin{verbatim}
    (A OR B OR ...) AND (X OR Y OR ...) AND ...
\end{verbatim}
Note that the i2b2 resource does not support query nesting.

% i2b2 parse and or:
% Y OR Z OR (A AND B)
% (Y OR Z OR A) AND (Y OR Z OR B)

\paragraph{tranSMART 17.1}
\label{sec:irct-res-transmart-17.1}

We make the tranSMART 17.1 resource declare the following \emph{select} operations:
\begin{itemize}
    \item \verb|AGGREGATE|, fields:
    \begin{itemize}
        \item \verb|FUNCTION|, mandatory, possible values:
        \verb|COUNT|,
        \verb|MIN|,
        \verb|MAX|,
        \verb|VALUES|
        
        \item \verb|DIMENSION|, optional, possible values:
        \verb|observation|,
        \verb|patient|,
        \verb|study|,
        \verb|trial_visit|
    \end{itemize}
    
    \item \verb|EXPORT|, fields:
    \begin{itemize}
        \item \verb|SUPPORTED_FORMATS|: possible values: \verb|file|, \verb|data|
        \item \verb|file|: value among the ones returns with \verb|SUPPORTED_FORMAT|
    \end{itemize}
    
    \item (without predicate): select data to export
\end{itemize}

And the following \emph{where} predicates:
\begin{itemize}
    \item \verb|CONSTRAIN_CONCEPT|, constraint based on a concept, field:
    \verb|OPERATOR|, optional, possible values:
    \begin{itemize}
        \item numeric values: \verb|==|, \verb|<=|, \verb|>=|, \verb|<|, \verb|>|
        \item string values: \verb|LIKE[exact]|, \verb|LIKE[begin]|, \verb|LIKE[end]|, \verb|LIKE[contains]|
    \end{itemize}

    \item \verb|CONSTRAIN_FIELD|, constraint based on a field in one of the dimension, fields:
    \begin{itemize}
        \item \verb|DIMENSION|
        \item \verb|FIELD|
        \item \verb|OPERATOR|
        \item \verb|VALUE|
    \end{itemize}
    
    \item \verb|CONSTRAIN_PEDIGREE|, constraint based on relationship between patients, fields:
    \begin{itemize}
        \item \verb|TYPE|, e.g. \verb|PARENTS_OF|, etc. among permitted values
        \item \verb|BIOLOGICAL|, possible values: \verb|YES|, \verb|NO|, \verb|BOTH|
        \item \verb|CONSTRAINT_END|
    \end{itemize}
    
    \item \verb|CONSTRAINT_PATIENT_SET|, constraint based on a patient set, fields:
    \begin{itemize}
        \item \verb|PATIENT_SET_ID|: identifier of a patient set stored in the back end
        \item \verb|PATIENT_IDS|: array of patient identifiers
    \end{itemize}
    
    \item \verb|NESTING|, fields: \verb|TYPE| (values: \verb|start| or \verb|end|)
    
\end{itemize}

% lib
First in order to support all the calls to tranSMART 17.1 backends that are needed, a simple client library for tranSMART REST API v2 is developed.
In terms of features, this library offers all calls that were previously available in Glowing Bear, and that will be used to answer the calls from the PIC-SURE API.

% tree
The tree exposed through the PIC-SURE API is the same as the tranSMART 17.1.
Natively PIC-SURE does not support browsing the tree at more than one level at a time, so we add another relationship supported in browsing the tree: \verb|CHILDREN-DEPTH-X|, that will return children with a depth up to \verb|X|.
The visual attributes parameters is added in order to display with the proper type in the UI the node, an example of this is the study: it is a \verb|STUDY| node in the tree, and in order to be displayed as such in the UI the visual attributes parameters is set as such.

% queries: select / where
All the predicates and operations described before are translated into the native API in a straightforward way, as all the information for input is available.
The is a special case: if the queried concept belongs to a study, a study constraint is added, while if the concept is cross-study it is left as is.
The nesting of queries described section~\ref{sec:gb-logical-operators} is implemented in the resource and translated in the native API.

%todo: construction of query: need to fetch in tree the constraint to be used

\paragraph{SHRINE}
The SHRINE API is basically a limited i2b2 API (saves for some very minor additional fields).
It supports only counts, which means no data exports.
The only significant difference is in the results, as one query brings several results.
We take the i2b2 implementation and slightly modify to the purpose of being compatible with SHRINE nodes.

The resource exposes the following \emph{select} operations:
\begin{itemize}
    \item \verb|AGGREGATE|, fields:
    \begin{itemize}
        \item \verb|FUNCTION|, mandatory, possible values:
        \verb|COUNT|
        
        \item \verb|DIMENSION|, optional, possible values:
        \verb|observation|,
        \verb|patient|
    \end{itemize}
\end{itemize}

And the following \emph{where} predicates:
\begin{itemize}
    \item \verb|CONSTRAIN_CONCEPT|, constraint based on a concept, field:
    \verb|OPERATOR|, optional, possible values:
    \begin{itemize}
        \item numeric values: \verb|==|, \verb|<=|, \verb|>=|, \verb|<|, \verb|>|
        \item string values: \verb|LIKE[exact]|, \verb|LIKE[begin]|, \verb|LIKE[end]|, \verb|LIKE[contains]|
    \end{itemize}
\end{itemize}

%todo: \paragraph{tranSMART 16.2} it is not tranSMART 16.2 :( but i2b2 transmart

\subsection{Back Ends Modifications}
\subsubsection{i2b2 \& derivatives}
% todo: class to be implemented i saw about: https://github.com/i2b2/i2b2-core-server/tree/master/edu.harvard.i2b2.pm/src/edu/harvard/i2b2/pm/util
TBD: modify PM to support OIDC (will support medco and shrine as well)
add impl. for check token, considered as a simple pw
elaborate on this subject
set istoken to true
SessionKey: is the tag for i2b2 session
user mgmt?

\subsubsection{tranSMART 17.1}
% todo
TBD: using spring security, get OIDC compatibility
authorizations?


% todo: CHILD_DEPTH: how it works (decided server side, incremental, etc.)
% todo: implementation specifics, i2b2 cache, only project/categories level
% todo: i2b2 modifiers: todo
% i2b2: not possible to query 2 projects at same time